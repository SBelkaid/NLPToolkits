{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find all explicit connective information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding all explicit connectives from the gold data (`relations.json`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "from nltk.tree import Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set correct filenames and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parsefile = \"conll16st-en-01-12-16-trial/parses.json\"\n",
    "relationfile = \"conll16st-en-01-12-16-trial/relations.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_part_of_speech(docID, find_word, span):\n",
    "    \"\"\"\n",
    "    Get PoS from the parses.json file by filename, word and span.\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(parsefile, encoding=\"utf-8\") as infile:\n",
    "        parse_dict = json.load(infile)\n",
    "        \n",
    "        #enumerate to get sentenceID\n",
    "        sentencelist = enumerate(parse_dict[docID][\"sentences\"])\n",
    "        \n",
    "        for sentenceID, sentence in sentencelist:\n",
    "            for word_data in sentence[\"words\"]:\n",
    "                \n",
    "                word = word_data[0]                \n",
    "                off_begin = word_data[1][\"CharacterOffsetBegin\"]\n",
    "                off_end = word_data[1][\"CharacterOffsetEnd\"]\n",
    "                part_of_speech = word_data[1][\"PartOfSpeech\"]\n",
    "                \n",
    "                begin, end = span\n",
    "                \n",
    "                if off_begin == begin and off_end == end:\n",
    "                    return part_of_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_phrase_structure(docID, sentenceID):\n",
    "    \"\"\"\n",
    "    Retrieve phrase_structure from the parses.json file by filename and sentenceID.\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(parsefile, encoding=\"utf-8\") as infile:\n",
    "        parse_dict = json.load(infile)\n",
    "        \n",
    "    sentencelist = parse_dict[docID][\"sentences\"]\n",
    "    \n",
    "    phrase_structure = sentencelist[sentenceID][\"parsetree\"]\n",
    "    \n",
    "    return phrase_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_connective_dependency(docID, sentenceID, part_of_speech, connective):\n",
    "    \"\"\"\n",
    "    Return heading and attached dependencies of connectives. \n",
    "    \"\"\"\n",
    "\n",
    "    dependency_heading = \"_\"\n",
    "    dependency_attached = \"_\"\n",
    "    \n",
    "    with open(parsefile, encoding=\"utf-8\") as infile:\n",
    "        parse_dict = json.load(infile)\n",
    "        \n",
    "    sentencelist = parse_dict[docID][\"sentences\"]\n",
    "    \n",
    "    dependencylist = sentencelist[sentenceID][\"dependencies\"]\n",
    "    \n",
    "    for dependency in dependencylist:\n",
    "        pos, heading, attached = dependency\n",
    "        \n",
    "        heading_token, headingID = heading.split(\"-\")[:2]\n",
    "        attached_token, attachedID = attached.split(\"-\")[:2]\n",
    "        \n",
    "        if attached_token == connective:\n",
    "            dependency_heading = heading\n",
    "            \n",
    "        if heading_token == connective:\n",
    "            dependency_attached = attached\n",
    "            \n",
    "    return dependency_heading, dependency_attached      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are only focussing on Arg1-arguments in the immediate previous sentence from the connective or the same sentence as the connective, this function returns `PS` or `SS` and `None` if the argument is further away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PS_or_SS(arg1_sentenceID, sentenceID):\n",
    "    \"\"\"\n",
    "    Returns Arg1 type (PS or SS) based on sentenceIDs.\n",
    "    \"\"\"\n",
    "\n",
    "    if arg1_sentenceID == sentenceID:\n",
    "        return \"SS\"\n",
    "    elif arg1_sentenceID == sentenceID-1:\n",
    "        return \"PS\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wsj_1000 160 5 0 But [879, 882] CC moved-13 _ [783, 877] PS [883, 957] Comparison.Contrast 1\n",
      "wsj_1000 412 13 7 and [2227, 2230] CC work-6 _ [2197, 2225] SS [2231, 2265] Expansion.Conjunction 7\n",
      "wsj_1000 421 14 0 While [2269, 2274] IN explained-4 _ [2389, 2445] SS [2275, 2386] Comparison.Concession 8\n",
      "wsj_1000 471 15 14 because [2518, 2525] IN ruining-18 _ [2493, 2517] SS [2526, 2552] Contingency.Cause.Reason 10\n",
      "wsj_1000 486 16 8 so [2577, 2579] IN said-2 _ [2554, 2573] SS [2580, 2636] Contingency.Cause.Result 12\n",
      "wsj_1000 502 17 1 also [2648, 2652] RB blasted-3 _ [9, 179] None [2641, 2647] Expansion.Conjunction 13\n",
      "wsj_1000 558 20 3 also [2939, 2943] RB has-5 _ [2878, 2923] PS [2925, 2938] Expansion.Conjunction 16\n",
      "wsj_1000 632 22 20 but [3332, 3335] CC is-7 _ [3282, 3330] SS [3336, 3367] Comparison.Contrast 18\n",
      "wsj_1000 677 24 9 but [3545, 3548] CC disturbs-7 _ [3507, 3543] SS [3549, 3654] Comparison.Contrast 20\n",
      "wsj_1000 765 28 6 when [4004, 4008] WRB is-10 _ [3974, 4003] SS [4009, 4035] Contingency.Condition 23\n",
      "wsj_1000 772 28 13 and [4037, 4040] CC Oct.-24 _ [3974, 4035] SS [4041, 4057] Expansion.Conjunction 24\n",
      "wsj_1000 877 32 0 But [4557, 4560] CC important-18 _ [4485, 4555] PS [4561, 4650] Comparison.Concession 27\n",
      "wsj_1000 878 32 1 if then [4561, 4563] IN _ _ [4564, 4610] SS [4557, 4560] Contingency.Condition 28\n"
     ]
    }
   ],
   "source": [
    "#To find all the connectives and store them in a list\n",
    "connectivelist = []\n",
    "\n",
    "#iterate through the file:\n",
    "with open(relationfile, encoding=\"utf-8\") as infile:\n",
    "\n",
    "    for relationID, json_line in enumerate(infile):\n",
    "        relation = json.loads(json_line)\n",
    "        \n",
    "        connective = relation[\"Connective\"][\"RawText\"]\n",
    "        connective_type = relation[\"Type\"]\n",
    "        \n",
    "        #only for explicit connectives\n",
    "        if connective_type != \"Explicit\":\n",
    "            continue\n",
    "            \n",
    "        docID = relation[\"DocID\"]       \n",
    "        \n",
    "        tokenlist = relation[\"Connective\"][\"TokenList\"]\n",
    "        sense = relation[\"Sense\"][0]\n",
    "        \n",
    "        connective_extent = relation[\"Connective\"][\"CharacterSpanList\"][0]\n",
    "        tokenID = tokenlist[0][2]\n",
    "        sentenceID = tokenlist[0][3]\n",
    "        sentence_position = tokenlist[0][4]\n",
    "        \n",
    "        arg1 = relation[\"Arg1\"]    \n",
    "        arg1_extent = arg1[\"CharacterSpanList\"][0]\n",
    "        arg1_sentenceID = arg1[\"TokenList\"][0][3]\n",
    "        arg1_type = PS_or_SS(arg1_sentenceID, sentenceID) #get arg1_type\n",
    "        \n",
    "        # if Arg1 is of type 'PS', give the phrase structure tree of its sentenceID\n",
    "        if arg1_type == \"PS\":\n",
    "            arg1_phrase_structure = get_phrase_structure(docID, arg1_sentenceID)\n",
    "        else:\n",
    "            arg1_phrase_structure = None\n",
    "        \n",
    "        arg2 = relation[\"Arg2\"]\n",
    "        arg2_extent = arg2[\"CharacterSpanList\"][0]\n",
    "        arg2_sentenceID = arg2[\"TokenList\"][0][3]\n",
    "        \n",
    "        \n",
    "        part_of_speech = get_part_of_speech(docID, connective, connective_extent)\n",
    "        \n",
    "        phrase_structure = get_phrase_structure(docID, sentenceID)\n",
    "        \n",
    "        dependency_heading, dependency_attached = get_connective_dependency(docID, sentenceID, part_of_speech, connective)\n",
    "        \n",
    "        connectivelist.append(connective.lower())\n",
    "        \n",
    "        \n",
    "        #OUTPUT, this somewhat resembles the format in the gdocs file\n",
    "        print(\n",
    "            docID, #filename\n",
    "            tokenID, #unique token ID\n",
    "            sentenceID, #sentenceID\n",
    "            sentence_position, #position in sentence\n",
    "            connective, #token\n",
    "            connective_extent, #extent connective\n",
    "            part_of_speech, #PoS\n",
    "#           lemmas\n",
    "            dependency_heading, #what is the head of the connective\n",
    "            dependency_attached, #what is attached to the connective\n",
    "#           phrase_structure, #phrase structure\n",
    "            arg1_extent, #extent arg1\n",
    "            arg1_type, #PS or SS\n",
    "#           arg1_phrase_structure, #arg1 phrase structure\n",
    "            arg2_extent, #extent arg2\n",
    "            sense, #meaning\n",
    "            relationID #discourse relation ID\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## List of explicit connectives and their frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'also': 2,\n",
       "         'and': 2,\n",
       "         'because': 1,\n",
       "         'but': 4,\n",
       "         'if then': 1,\n",
       "         'so': 1,\n",
       "         'when': 1,\n",
       "         'while': 1})"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connectives and frequencies\n",
    "connectives = Counter(connectivelist)\n",
    "\n",
    "connectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
