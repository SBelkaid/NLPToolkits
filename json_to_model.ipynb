{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import nltk\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Tuplelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tuplebuilder(features_json):\n",
    "    \"\"\"\n",
    "    Makes a tuplelist and corresponding featurelist\n",
    "    from an inputted features.json file. \n",
    "    \n",
    "    Outputs a tuplelist and a featurelist in a list for every sentence. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    with open(features_json, encoding='utf8') as features_file:\n",
    "        features = json.load(features_file)\n",
    "\n",
    "    tuplelist = []\n",
    "    featurelist = []\n",
    "\n",
    "    for docID in features.keys():\n",
    "        for sentence_n in features[docID][\"Sentences\"]:\n",
    "\n",
    "            #reset content of tokenlist\n",
    "            tokenlist = []\n",
    "            sentencetokenfeatures = []\n",
    "\n",
    "            for token in features[docID][\"Sentences\"][sentence_n]:\n",
    "                tokenfeatures = dict()\n",
    "\n",
    "                #importing variables from tokeninfo\n",
    "                for feature, value in token.items():\n",
    "                    if feature != \"label\":\n",
    "                        tokenfeatures[feature] = value\n",
    "\n",
    "                tokenstring = token[\"token\"]\n",
    "                argpart = token[\"label\"]\n",
    "\n",
    "                tokentuple = (tokenstring, argpart)    \n",
    "\n",
    "                tokenlist.append(tokentuple)           \n",
    "                sentencetokenfeatures.append(tokenfeatures)\n",
    "\n",
    "            #Building tuple\n",
    "            tuplelist.append(tokenlist)\n",
    "            featurelist.append(sentencetokenfeatures)\n",
    "            \n",
    "    return tuplelist, featurelist\n",
    "            \n",
    "#tuplelist, featurelist = tuplebuilder(\"features_arg1_SS.json\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def arg_features(sentence, i, history, n_sent):\n",
    "    \n",
    "    features = featurelist[n_sent][i]\n",
    "    #print(features)\n",
    "    \n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "        features[\"prev-tag\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "        features[\"prev-tag\"] = history[i-1]\n",
    "        \n",
    "#     print(features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConsecutiveArgTagger(nltk.TaggerI):\n",
    "\n",
    "    def __init__(self, train_sents):\n",
    "        train_set = []\n",
    "        for n_sent, tagged_sent in enumerate(train_sents):\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "#             print(untagged_sent)\n",
    "            history = []\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = arg_features(untagged_sent, i, history, n_sent)\n",
    "                train_set.append( (featureset, tag) )\n",
    "                history.append(tag)\n",
    "        self.classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        \n",
    "        global count\n",
    "        \n",
    "        history = []\n",
    "#         print(sentence,\"\\n\")\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = arg_features(sentence, i, history, count)\n",
    "            tag = self.classifier.classify(featureset)\n",
    "            history.append(tag)\n",
    "            \n",
    "        count += 1 #for the sentenceID\n",
    "        \n",
    "        return list(zip(sentence, history))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Magic, creating the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tuplelist, featurelist = tuplebuilder(\"features_arg1_SS.json\")\n",
    "tagger = ConsecutiveArgTagger(tuplelist)\n",
    "pickle.dump(tagger, open(\"models/tagger_arg1_SS.pickle\", \"wb\"))\n",
    "pickle.dump(((tuplelist,featurelist)), open(\"models/tuplelist_featurelist_arg1_SS.pickle\", \"wb\"))\n",
    "\n",
    "tuplelist, featurelist = tuplebuilder(\"features_arg1_PS.json\")\n",
    "tagger = ConsecutiveArgTagger(tuplelist)\n",
    "pickle.dump(tagger, open(\"models/tagger_arg1_PS.pickle\", \"wb\"))\n",
    "pickle.dump(((tuplelist,featurelist)), open(\"models/tuplelist_featurelist_arg1_PS.pickle\", \"wb\"))\n",
    "\n",
    "tuplelist, featurelist = tuplebuilder(\"features_arg2.json\")\n",
    "tagger = ConsecutiveArgTagger(tuplelist)\n",
    "pickle.dump(tagger, open(\"models/tagger_arg2.pickle\", \"wb\"))\n",
    "pickle.dump(((tuplelist,featurelist)), open(\"models/tuplelist_featurelist_arg2.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for testing\n",
    "tuplelist, featurelist = tuplebuilder(\"features_arg2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 0 #reset the sentenceID\n",
    "size = int(len(tuplelist) * 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sents, test_sents = tuplelist[size:], tuplelist[:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7138628309360017\n"
     ]
    }
   ],
   "source": [
    "print(tagger.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tagger.tag_sents(test_sents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
