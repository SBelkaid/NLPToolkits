{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I worked with this notbeook inside the development data folder, so if you would like to run the code, download it and put it in the right folder (or change the paths in the notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "import json\n",
    "import codecs\n",
    "from nltk.tree import Tree\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Specify path to data to be analyzed (I entered the path to the tutorial data for now, so that the code runs)\n",
    "\n",
    "with codecs.open ('tutorial/conll16st-en-01-12-16-trial/relations.json', 'r', encoding = 'utf-8') as pdtb_file:\n",
    "    \n",
    "\n",
    "    # Assign all relations (a list) to a variable\n",
    "    relations = [json.loads(x) for x in pdtb_file];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "{'Arg1': {'CharacterSpanList': [[783, 877]],\n",
      "          'RawText': 'Several big securities firms backed off from program '\n",
      "                     'trading a few months after the 1987 crash',\n",
      "          'TokenList': [[783, 790, 143, 4, 0],\n",
      "                        [791, 794, 144, 4, 1],\n",
      "                        [795, 805, 145, 4, 2],\n",
      "                        [806, 811, 146, 4, 3],\n",
      "                        [812, 818, 147, 4, 4],\n",
      "                        [819, 822, 148, 4, 5],\n",
      "                        [823, 827, 149, 4, 6],\n",
      "                        [828, 835, 150, 4, 7],\n",
      "                        [836, 843, 151, 4, 8],\n",
      "                        [844, 845, 152, 4, 9],\n",
      "                        [846, 849, 153, 4, 10],\n",
      "                        [850, 856, 154, 4, 11],\n",
      "                        [857, 862, 155, 4, 12],\n",
      "                        [863, 866, 156, 4, 13],\n",
      "                        [867, 871, 157, 4, 14],\n",
      "                        [872, 877, 158, 4, 15]]},\n",
      " 'Arg2': {'CharacterSpanList': [[883, 957]],\n",
      "          'RawText': 'most of them, led by Morgan Stanley & Co., moved back in '\n",
      "                     'earlier this year',\n",
      "          'TokenList': [[883, 887, 161, 5, 1],\n",
      "                        [888, 890, 162, 5, 2],\n",
      "                        [891, 895, 163, 5, 3],\n",
      "                        [895, 896, 164, 5, 4],\n",
      "                        [897, 900, 165, 5, 5],\n",
      "                        [901, 903, 166, 5, 6],\n",
      "                        [904, 910, 167, 5, 7],\n",
      "                        [911, 918, 168, 5, 8],\n",
      "                        [919, 920, 169, 5, 9],\n",
      "                        [921, 924, 170, 5, 10],\n",
      "                        [924, 925, 171, 5, 11],\n",
      "                        [926, 931, 172, 5, 12],\n",
      "                        [932, 936, 173, 5, 13],\n",
      "                        [937, 939, 174, 5, 14],\n",
      "                        [940, 947, 175, 5, 15],\n",
      "                        [948, 952, 176, 5, 16],\n",
      "                        [953, 957, 177, 5, 17]]},\n",
      " 'Connective': {'CharacterSpanList': [[879, 882]],\n",
      "                'RawText': 'But',\n",
      "                'TokenList': [[879, 882, 160, 5, 0]]},\n",
      " 'DocID': 'wsj_1000',\n",
      " 'ID': 14878,\n",
      " 'Sense': ['Comparison.Contrast'],\n",
      " 'Type': 'Explicit'}\n"
     ]
    }
   ],
   "source": [
    "# Loop through relations and select only explicit relations:\n",
    "\n",
    "# List for explicit relations:\n",
    "\n",
    "relations_explicit = []\n",
    "\n",
    "for relation in relations:\n",
    "    if relation['Type'] == 'Explicit':\n",
    "        relations_explicit.append(relation)\n",
    "        \n",
    "print(len(relations_explicit))\n",
    "pprint.pprint(relations_explicit[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# Split in two lists according to arg1 in same sentence and arg1 in previous sentence:\n",
    "\n",
    "relations_ss = []\n",
    "relations_ps = []\n",
    "relations_other = []\n",
    "\n",
    "for relation in relations_explicit:\n",
    "    sentence_id_arg1 = relation['Arg1']['TokenList'][0][3]\n",
    "    sentence_id_connective = relation['Connective']['TokenList'][0][3]\n",
    "    sentence_id_arg2 = relation['Arg2']['TokenList'][0][3]\n",
    "    \n",
    "    if sentence_id_arg1 == sentence_id_connective == sentence_id_arg2:\n",
    "        relations_ss.append(relation)\n",
    "    elif int(sentence_id_arg1) == int(sentence_id_connective) - 1 == int(sentence_id_arg2) -1:\n",
    "        relations_ps.append(relation)\n",
    "    else: \n",
    "        relations_other.append(relation)\n",
    "print(len(relations_ps))\n",
    "print(len(relations_ss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Open parse file:\n",
    "\n",
    "with codecs.open('tutorial/conll16st-en-01-12-16-trial/parses.json', 'r', encoding = 'utf8') as parse_file:\n",
    "    parses = json.load(parse_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We would stop index arbitrage when the market is under stress\n",
      "We would stop index arbitrage when the market is under stress\n",
      "rtc's! Work didn t doesn t $50\n"
     ]
    }
   ],
   "source": [
    "# NOT NEEDED \n",
    "\n",
    "def normalize_str(text):\n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    text = text.lstrip('(')\n",
    "    \n",
    "    text = text.rstrip(')')\n",
    "    text_list = text.split()\n",
    "    \n",
    "    clean_text_list = []\n",
    "    \n",
    "    for word in text_list:\n",
    "        word = word.strip()\n",
    "        clean_text_list.append(word)\n",
    "    \n",
    "    clean_text = ' '.join(clean_text_list)\n",
    "    \n",
    "    return(clean_text)\n",
    "\n",
    "my_rel = relations_ss[7]\n",
    "\n",
    "arg1_str = my_rel['Arg1']['RawText']\n",
    "print(arg1_str)\n",
    "print(normalize_str(arg1_str))\n",
    "print(normalize_str(\"(rtc's! Work\\n didn t doesn t $50\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_const_type(constituent_list, constituent_name):\n",
    "    \"\"\"\n",
    "    Input: set of constituents, name of a constituent type (str)\n",
    "    Output: list of constituents of the selected type \n",
    "    \"\"\"\n",
    "    const_type_list = []\n",
    "    \n",
    "    for constituent in constituent_list:\n",
    "        \n",
    "        \n",
    "        if constituent.startswith('('+constituent_name):\n",
    "            const_type_list.append(constituent)\n",
    "    \n",
    "    return const_type_list\n",
    "            \n",
    "# E.g. get clauses:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_subtrees(parsed_sentence, const_type = 0):\n",
    "    \"\"\"\"\"\"\n",
    "    tree_nltk = Tree.fromstring(parsed_sentence)\n",
    "    \n",
    "    tree_height = tree_nltk.height()\n",
    "    \n",
    "        \n",
    "    const_tuple_list = []\n",
    "\n",
    "    for s in tree_nltk.subtrees():\n",
    "\n",
    "        label = s.label()\n",
    "        \n",
    "        if (label != '``') and (label != ''):\n",
    "            \n",
    "            subtree_tokens_str = str(s.flatten())\n",
    "            #print(type(subtree_tokens_str))\n",
    "            clean_subtree_str = normalize_str(subtree_tokens_str).lstrip(label).lstrip(' ')\n",
    "\n",
    "\n",
    "\n",
    "            if const_type != 0:\n",
    "                if label == const_type:\n",
    "                    const_tuple_list.append((label, clean_subtree_str))\n",
    "\n",
    "            else:\n",
    "\n",
    "                const_tuple_list.append((label, clean_subtree_str))\n",
    "\n",
    "\n",
    "\n",
    "    return const_tuple_list\n",
    "\n",
    "doc_id = my_rel['DocID']\n",
    "sentence_id_arg1 = my_rel['Arg1']['TokenList'][0][3]\n",
    "parsed_sentence = parses[doc_id]['sentences'][sentence_id_arg1]['parsetree']  \n",
    "\n",
    "consts = get_subtrees(parsed_sentence)\n",
    "\n",
    "#pprint.pprint(consts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wsj_1000 [0, 1, 2, 3, 4, 5]\n",
      "(14884, 'wsj_1000', 13, \"Index arbitrage doesn't work\", [1, 2, 3, 4, 5])\n",
      "(14885, 'wsj_1000', 14, 'he said that \"on an intraday basis, it has major effects', [22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34])\n",
      "(14887, 'wsj_1000', 15, 'and told them to cool it', [8, 9, 10, 11, 12, 13])\n",
      "(14889, 'wsj_1000', 16, 'They said, `Too bad', [0, 1, 2, 3, 4, 5])\n",
      "(14895, 'wsj_1000', 22, \"they'd love to do away with it {program trading}\", [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18])\n",
      "(14897, 'wsj_1000', 24, \"the market's volatility disturbs him\", [2, 3, 4, 5, 6, 7])\n",
      "(14900, 'wsj_1000', 28, 'We would stop index arbitrage', [1, 2, 3, 4, 5])\n",
      "(14901, 'wsj_1000', 28, 'We would stop index arbitrage when the market is under stress', [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])\n",
      "(14905, 'wsj_1000', 32, 'this prompts others to consider the same thing', [2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "def get_data(relation, arg):\n",
    "    \n",
    "    \"\"\"returns DocID, sentenceID, Argument raw text and sentence_token_id in relations.json\"\"\"\n",
    "    \n",
    "    relation_id = relation['ID']\n",
    "    doc_id = relation['DocID']\n",
    "    sentence_id_arg = relation[arg]['TokenList'][0][3]\n",
    "    arg_str = relation[arg]['RawText']   \n",
    "    \n",
    "    sentence_token_id_relations = []\n",
    "    \n",
    "    relations_token_list = relation[arg]['TokenList']\n",
    "    \n",
    "    for line in relations_token_list:\n",
    "        sentence_token_id = line[4]\n",
    "        sentence_token_id_relations.append(sentence_token_id)\n",
    "    \n",
    "    return relation_id, doc_id, sentence_id_arg, arg_str, sentence_token_id_relations\n",
    "\n",
    "my_rel = relations_ss[3]\n",
    "my_relid, my_docid, my_sid, my_arg, my_st = get_data(my_rel, 'Arg1')\n",
    "print(my_id, my_st)\n",
    "\n",
    "for rel in relations_ss:\n",
    "    print(get_data(rel, 'Arg1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_sentence_token_id(doc_id, sentence_id_arg, sentence_token_id_relations):\n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    argument_id_tuples_list = []\n",
    "    \n",
    "    parsed_words_list = parses[doc_id]['sentences'][sentence_id_arg]['words']\n",
    "    \n",
    "    for number, word in enumerate(parsed_words_list):\n",
    "        \n",
    "        if number in sentence_token_id_relations:\n",
    "        \n",
    "            argument_id_tuples_list.append((number, word[0]))\n",
    "        \n",
    "        \n",
    "    return argument_id_tuples_list\n",
    "\n",
    "my_tuple_list = parse_sentence_token_id('wsj_1000', 22, my_st)\n",
    "\n",
    "# Looks good, if you compare these ids to the sentence_token_ids taken from relations.json. \n",
    "# Covers the span of the argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"`` The impression I 've got\"]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_arg_list(argument_tuples_list):\n",
    "    \n",
    "    \"\"\"\"\"\"\n",
    "\n",
    "\n",
    "    # check if argument is interrupted by comparing to the sentence token ids in the relations.json file\n",
    "    \n",
    "    for index, pair in enumerate(argument_tuples_list):\n",
    "        if pair == argument_tuples_list[-1]:\n",
    "            break\n",
    "\n",
    "        next_pair = argument_tuples_list[index + 1]\n",
    "\n",
    "\n",
    "        if (next_pair[0] - pair[0]) > 1:\n",
    "            slice_index = index\n",
    "            \n",
    "            break\n",
    "        else:\n",
    "            slice_index = None\n",
    "    \n",
    "    argument_tokens_as_in_parsing = []\n",
    "    \n",
    "    for st_id, token in argument_tuples_list:\n",
    "        \n",
    "        argument_tokens_as_in_parsing.append(token)  \n",
    "    # Get the argument as either 1 string in a list (if it is not interrupted) or as 2 strings in a list\n",
    "    # if it is interrupted\n",
    "\n",
    "    if slice_index:\n",
    "        arg1_1 = ' '.join(argument_tokens_as_in_parsing[:slice_index + 1])\n",
    "\n",
    "        arg1_2 = ' '.join(argument_tokens_as_in_parsing[slice_index + 1:])\n",
    "\n",
    "        arg_str_list = [arg1_1, arg1_2]\n",
    "    else:\n",
    "\n",
    "        arg_str_list = [' '.join(argument_tokens_as_in_parsing)]\n",
    "    \n",
    "    \n",
    "    return arg_str_list\n",
    "\n",
    "get_arg_list(my_tuple_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Map arguments to constituents\n",
    "\n",
    "def constituent_structure(relation, arg):\n",
    "    \"\"\"\"\"\"\n",
    "\n",
    "    # Dicts with argument as key and constituent as value (tuple: (label, constituent))\n",
    "    arg_is_const = defaultdict(list)\n",
    "    arg_starts_const = defaultdict(list)\n",
    "    arg_in_const = defaultdict(list)\n",
    "    const_part_arg = defaultdict(list)\n",
    "    \n",
    "    \n",
    "    \n",
    "    rel_id, doc_id, sentence_id_arg, arg_str, sentence_token_id_relations = get_data(relation, arg)\n",
    "    \n",
    "    arg_tuples_list = my_tuple_list = parse_sentence_token_id(doc_id, sentence_id_arg, sentence_token_id_relations)\n",
    "    \n",
    "    arg_list = get_arg_list(arg_tuples_list)\n",
    "\n",
    "\n",
    "    sentence = parses[doc_id]['sentences'][sentence_id_arg]['parsetree']\n",
    "\n",
    "    const_list = get_subtrees(sentence)\n",
    "\n",
    "\n",
    "    for argument in arg_list:\n",
    "\n",
    "\n",
    "\n",
    "        for constituent in const_list:\n",
    "\n",
    "            label, const = constituent\n",
    "    \n",
    "\n",
    "\n",
    "            # Constituents the argument is part of:\n",
    "\n",
    "            if const == argument:\n",
    "\n",
    "                arg_is_const[argument].append((label, const))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            elif const.startswith(argument):\n",
    "\n",
    "                arg_starts_const[argument].append((label, const))\n",
    "\n",
    "\n",
    "            elif argument in const:\n",
    "\n",
    "                arg_in_const[argument].append((label, const))\n",
    "\n",
    "\n",
    "\n",
    "            # Constituents that are part of the argument\n",
    "\n",
    "            if const in argument:\n",
    "                const_part_arg[argument].append((label, const))\n",
    "                \n",
    "    return rel_id, doc_id, arg_is_const, arg_starts_const, arg_in_const, const_part_arg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S Index arbitrage does n't work\n",
      "S They said , ` Too bad\n",
      "SBAR they 'd love to do away with it -LCB- program trading -RCB-\n",
      "S they 'd love to do away with it -LCB- program trading -RCB-\n",
      "SBAR the market 's volatility disturbs him\n",
      "S the market 's volatility disturbs him\n",
      "S We would stop index arbitrage when the market is under stress\n",
      "S this prompts others to consider the same thing\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('arg_constituents.tsv', 'w', encoding = 'utf-8') as outfile:   \n",
    "    \n",
    "    for rel in relations_ss:\n",
    "\n",
    "        rel_id, doc_id, dict1, dict2, dict3, dict4 = constituent_structure(rel, 'Arg1')\n",
    "\n",
    "        if dict1:\n",
    "        \n",
    "            for arg, const in dict1.items():\n",
    "                for l, c in const:\n",
    "                    print(l, c)\n",
    "                    outfile.write('same sentence\\t'+str(rel_id)+'\\t'+str(doc_id)+'\\t'+arg+'\\t'+l+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {})\n",
      "defaultdict(<class 'list'>, {'he said that `` on an intraday basis , it has major effects': [('NP', 'he'), ('PRP', 'he'), (',', ','), ('NP', 'he'), ('PRP', 'he'), ('VP', 'said that `` on an intraday basis , it has major effects'), ('VBD', 'said'), ('SBAR', 'that `` on an intraday basis , it has major effects'), ('IN', 'that'), ('S', 'on an intraday basis , it has major effects'), ('PP', 'on an intraday basis'), ('IN', 'on'), ('NP', 'an intraday basis'), ('DT', 'an'), ('JJ', 'intraday'), ('NN', 'basis'), (',', ','), ('NP', 'it'), ('PRP', 'it'), ('VP', 'has major effects'), ('VBZ', 'has'), ('NP', 'major effects'), ('JJ', 'major'), ('NNS', 'effects')]})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if dict1:\n",
    "    \n",
    "    print('yes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {\"they 'd love to do away with it -LCB- program trading -RCB-\": [('SBAR', \"they 'd love to do away with it -LCB- program trading -RCB-\"), ('S', \"they 'd love to do away with it -LCB- program trading -RCB-\"), ('NP', 'they'), ('PRP', 'they'), ('VP', \"'d love to do away with it -LCB- program trading -RCB-\"), ('MD', \"'d\"), ('VP', 'love to do away with it -LCB- program trading -RCB-'), ('VB', 'love'), ('S', 'to do away with it -LCB- program trading -RCB-'), ('VP', 'to do away with it -LCB- program trading -RCB-'), ('TO', 'to'), ('VP', 'do away with it -LCB- program trading -RCB-'), ('VB', 'do'), ('ADVP', 'away with it'), ('RB', 'away'), ('PP', 'with it'), ('IN', 'with'), ('NP', 'it'), ('PRP', 'it'), ('PRN', '-LCB- program trading -RCB-'), ('-LRB-', '-LCB-'), ('NP', 'program trading'), ('NN', 'program'), ('NN', 'trading'), ('-RRB-', '-RCB-'), ('NP', 'they'), ('PRP', 'they'), ('-LRB-', '-LCB-'), ('DT', 'the'), ('-RRB-', '-RCB-'), ('VB', 'do'), ('NP', 'it'), ('PRP', 'it'), ('NP', 'he'), ('PRP', 'he')]})\n"
     ]
    }
   ],
   "source": [
    "print(dict4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {\"they 'd love to do away with it -LCB- program trading -RCB-\": [('S', \"`` The impression I 've got is they 'd love to do away with it -LCB- program trading -RCB- , but they -LCB- the exchange -RCB- ca n't do it , '' he said .\"), ('S', \"The impression I 've got is they 'd love to do away with it -LCB- program trading -RCB- , but they -LCB- the exchange -RCB- ca n't do it\"), ('S', \"The impression I 've got is they 'd love to do away with it -LCB- program trading -RCB-\"), ('VP', \"is they 'd love to do away with it -LCB- program trading -RCB-\")]})\n"
     ]
    }
   ],
   "source": [
    "print(dict3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'said', 'that', '``', 'on', 'an', 'intraday', 'basis', ',', 'it', 'has', 'major', 'effects']\n",
      "(12, ['said', 'that', '``', 'on', 'an', 'intraday', 'basis', ',', 'it', 'has', 'major', 'effects'], 'VP')\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Can't convert 'list' object to str implicitly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-63ddb959ce77>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mrest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlongest_const\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[0mrest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlongest_const\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Can't convert 'list' object to str implicitly"
     ]
    }
   ],
   "source": [
    "arg = list(dict4.keys())\n",
    "\n",
    "arg = arg[0]\n",
    "arg_list = arg.split()\n",
    "const_list = list(dict4.values())\n",
    "\n",
    "const_str_list = []\n",
    "\n",
    "for const in const_list[0]:\n",
    "    label, c = const\n",
    "    c = c.split()\n",
    "    const_str_list.append((len(c), c, label))\n",
    "    \n",
    "longest_const = (max(const_str_list))  \n",
    "print(arg_list)\n",
    "print(longest_const)\n",
    "\n",
    "\n",
    "for word_arg in arg:\n",
    "    for word_const in longest_co\n",
    "        if word_arg not in \n",
    "    \n",
    "\n",
    "rest = arg.replace(longest_const[1], '')\n",
    "rest = rest.strip(' ')\n",
    "print(longest_const)\n",
    "print(arg)\n",
    "pprint.pprint(rest)\n",
    "\n",
    "for const in const_list[0]:\n",
    "    label, c = const\n",
    "    if rest == c:\n",
    "        print(const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
