{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I worked with this notbeook inside the development data folder, so if you would like to run the code, download it and put it in the right folder (or change the paths in the notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "import json\n",
    "import codecs\n",
    "from nltk.tree import Tree\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Specify path to data to be analyzed (I entered the path to the tutorial data for now, so that the code runs)\n",
    "\n",
    "with codecs.open ('relations.json', 'r', encoding = 'utf-8') as pdtb_file:\n",
    "    \n",
    "\n",
    "    # Assign all relations (a list) to a variable\n",
    "    relations = [json.loads(x) for x in pdtb_file];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14722\n",
      "{'Arg1': {'CharacterSpanList': [[825, 961]],\n",
      "          'RawText': 'The new \"social choice\" fund will shun securities of '\n",
      "                     'companies linked to South Africa, nuclear power and in '\n",
      "                     'some cases, Northern Ireland',\n",
      "          'TokenList': [[825, 828, 145, 6, 0],\n",
      "                        [829, 832, 146, 6, 1],\n",
      "                        [833, 834, 147, 6, 2],\n",
      "                        [834, 840, 148, 6, 3],\n",
      "                        [841, 847, 149, 6, 4],\n",
      "                        [847, 848, 150, 6, 5],\n",
      "                        [849, 853, 151, 6, 6],\n",
      "                        [854, 858, 152, 6, 7],\n",
      "                        [859, 863, 153, 6, 8],\n",
      "                        [864, 874, 154, 6, 9],\n",
      "                        [875, 877, 155, 6, 10],\n",
      "                        [878, 887, 156, 6, 11],\n",
      "                        [888, 894, 157, 6, 12],\n",
      "                        [895, 897, 158, 6, 13],\n",
      "                        [898, 903, 159, 6, 14],\n",
      "                        [904, 910, 160, 6, 15],\n",
      "                        [910, 911, 161, 6, 16],\n",
      "                        [912, 919, 162, 6, 17],\n",
      "                        [920, 925, 163, 6, 18],\n",
      "                        [926, 929, 164, 6, 19],\n",
      "                        [930, 932, 165, 6, 20],\n",
      "                        [933, 937, 166, 6, 21],\n",
      "                        [938, 943, 167, 6, 22],\n",
      "                        [943, 944, 168, 6, 23],\n",
      "                        [945, 953, 169, 6, 24],\n",
      "                        [954, 961, 170, 6, 25]]},\n",
      " 'Arg2': {'CharacterSpanList': [[968, 1103]],\n",
      "          'RawText': 'excluded will be investments in companies with '\n",
      "                     '\"significant\" business stemming from weapons '\n",
      "                     'manufacture, alcoholic beverages or tobacco',\n",
      "          'TokenList': [[968, 976, 173, 7, 1],\n",
      "                        [977, 981, 174, 7, 2],\n",
      "                        [982, 984, 175, 7, 3],\n",
      "                        [985, 996, 176, 7, 4],\n",
      "                        [997, 999, 177, 7, 5],\n",
      "                        [1000, 1009, 178, 7, 6],\n",
      "                        [1010, 1014, 179, 7, 7],\n",
      "                        [1015, 1016, 180, 7, 8],\n",
      "                        [1016, 1027, 181, 7, 9],\n",
      "                        [1027, 1028, 182, 7, 10],\n",
      "                        [1029, 1037, 183, 7, 11],\n",
      "                        [1038, 1046, 184, 7, 12],\n",
      "                        [1047, 1051, 185, 7, 13],\n",
      "                        [1052, 1059, 186, 7, 14],\n",
      "                        [1060, 1071, 187, 7, 15],\n",
      "                        [1071, 1072, 188, 7, 16],\n",
      "                        [1073, 1082, 189, 7, 17],\n",
      "                        [1083, 1092, 190, 7, 18],\n",
      "                        [1093, 1095, 191, 7, 19],\n",
      "                        [1096, 1103, 192, 7, 20]]},\n",
      " 'Connective': {'CharacterSpanList': [[963, 967]],\n",
      "                'RawText': 'Also',\n",
      "                'TokenList': [[963, 967, 172, 7, 0]]},\n",
      " 'DocID': 'wsj_0204',\n",
      " 'ID': 3179,\n",
      " 'Sense': ['Expansion.Conjunction'],\n",
      " 'Type': 'Explicit'}\n"
     ]
    }
   ],
   "source": [
    "# Loop through relations and select only explicit relations:\n",
    "\n",
    "# List for explicit relations:\n",
    "\n",
    "relations_explicit = []\n",
    "\n",
    "for relation in relations:\n",
    "    if relation['Type'] == 'Explicit':\n",
    "        relations_explicit.append(relation)\n",
    "        \n",
    "print(len(relations_explicit))\n",
    "pprint.pprint(relations_explicit[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4119\n",
      "8893\n"
     ]
    }
   ],
   "source": [
    "# Split in two lists according to arg1 in same sentence and arg1 in previous sentence:\n",
    "\n",
    "relations_ss = []\n",
    "relations_ps = []\n",
    "relations_other = []\n",
    "\n",
    "for relation in relations_explicit:\n",
    "    sentence_id_arg1 = relation['Arg1']['TokenList'][0][3]\n",
    "    sentence_id_connective = relation['Connective']['TokenList'][0][3]\n",
    "    sentence_id_arg2 = relation['Arg2']['TokenList'][0][3]\n",
    "    \n",
    "    if sentence_id_arg1 == sentence_id_connective == sentence_id_arg2:\n",
    "        relations_ss.append(relation)\n",
    "    elif int(sentence_id_arg1) == int(sentence_id_connective) - 1 == int(sentence_id_arg2) -1:\n",
    "        relations_ps.append(relation)\n",
    "    else: \n",
    "        relations_other.append(relation)\n",
    "print(len(relations_ps))\n",
    "print(len(relations_ss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Open parse file:\n",
    "\n",
    "with codecs.open('parses.json', 'r', encoding = 'utf8') as parse_file:\n",
    "    parses = json.load(parse_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and that it was his mother's birthday\n",
      "and that it was his mother's birthday\n",
      "rtc's! Work didn t doesn t $50\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def normalize_str(text):\n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    text = text.lstrip('(')\n",
    "    \n",
    "    text = text.rstrip(')')\n",
    "    text_list = text.split()\n",
    "    \n",
    "    clean_text_list = []\n",
    "    \n",
    "    for word in text_list:\n",
    "        word = word.strip()\n",
    "        clean_text_list.append(word)\n",
    "    \n",
    "    clean_text = ' '.join(clean_text_list)\n",
    "    \n",
    "    return(clean_text)\n",
    "\n",
    "my_rel = relations_ss[7]\n",
    "\n",
    "arg1_str = my_rel['Arg1']['RawText']\n",
    "print(arg1_str)\n",
    "print(normalize_str(arg1_str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_subtrees(parsed_sentence, const_type = 0):\n",
    "    \"\"\"\"\"\"\n",
    "    tree_nltk = Tree.fromstring(parsed_sentence)\n",
    "    \n",
    "    tree_height = tree_nltk.height()\n",
    "    \n",
    "        \n",
    "    const_tuple_list = []\n",
    "\n",
    "    for s in tree_nltk.subtrees():\n",
    "\n",
    "        label = s.label()\n",
    "        \n",
    "        if (label != '``') and (label != ''):\n",
    "            \n",
    "            subtree_tokens_str = str(s.flatten())\n",
    "            #print(type(subtree_tokens_str))\n",
    "            clean_subtree_str = normalize_str(subtree_tokens_str).lstrip(label).lstrip(' ')\n",
    "\n",
    "\n",
    "\n",
    "            if const_type != 0:\n",
    "                if label == const_type:\n",
    "                    const_tuple_list.append((label, clean_subtree_str))\n",
    "\n",
    "            else:\n",
    "\n",
    "                const_tuple_list.append((label, clean_subtree_str))\n",
    "\n",
    "\n",
    "\n",
    "    return const_tuple_list\n",
    "\n",
    "doc_id = my_rel['DocID']\n",
    "sentence_id_arg1 = my_rel['Arg1']['TokenList'][0][3]\n",
    "parsed_sentence = parses[doc_id]['sentences'][sentence_id_arg1]['parsetree']  \n",
    "\n",
    "consts = get_subtrees(parsed_sentence)\n",
    "\n",
    "#pprint.pprint(consts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3186, 'wsj_0207', 1, 'He commissions and splendidly interprets fearsome contemporary scores and does some conducting', [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24])\n"
     ]
    }
   ],
   "source": [
    "def get_data(relation, arg):\n",
    "    \n",
    "    \"\"\"returns DocID, sentenceID, Argument raw text and sentence_token_id in relations.json\"\"\"\n",
    "    \n",
    "    relation_id = relation['ID']\n",
    "    doc_id = relation['DocID']\n",
    "    sentence_id_arg = relation[arg]['TokenList'][0][3]\n",
    "    arg_str = relation[arg]['RawText']   \n",
    "    \n",
    "    sentence_token_id_relations = []\n",
    "    \n",
    "    relations_token_list = relation[arg]['TokenList']\n",
    "    \n",
    "    for line in relations_token_list:\n",
    "        sentence_token_id = line[4]\n",
    "        sentence_token_id_relations.append(sentence_token_id)\n",
    "    \n",
    "    return relation_id, doc_id, sentence_id_arg, arg_str, sentence_token_id_relations\n",
    "\n",
    "my_rel = relations_ss[3]\n",
    "my_relid, my_docid, my_sid, my_arg, my_st = get_data(my_rel, 'Arg1')\n",
    "\n",
    "\n",
    "print(get_data(my_rel, 'Arg1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(13, 'He'), (14, 'commissions'), (15, 'and'), (16, 'splendidly'), (17, 'interprets'), (18, 'fearsome'), (19, 'contemporary'), (20, 'scores'), (21, 'and'), (22, 'does'), (23, 'some'), (24, 'conducting')]\n"
     ]
    }
   ],
   "source": [
    "def parse_sentence_token_id(doc_id, sentence_id_arg, sentence_token_id_relations):\n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    argument_id_tuples_list = []\n",
    "    \n",
    "    parsed_words_list = parses[doc_id]['sentences'][sentence_id_arg]['words']\n",
    "    \n",
    "    for number, word in enumerate(parsed_words_list):\n",
    "        \n",
    "        if number in sentence_token_id_relations:\n",
    "        \n",
    "            argument_id_tuples_list.append((number, word[0]))\n",
    "        \n",
    "        \n",
    "    return argument_id_tuples_list\n",
    "\n",
    "my_tuple_list = parse_sentence_token_id(my_docid, my_sid, my_st)\n",
    "print(my_tuple_list)\n",
    "# Looks good, if you compare these ids to the sentence_token_ids taken from relations.json. \n",
    "# Covers the span of the argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He commissions and splendidly interprets fearsome contemporary scores and does some conducting']"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_arg_list(argument_tuples_list):\n",
    "    \n",
    "    \"\"\"\"\"\"\n",
    "\n",
    "\n",
    "    # check if argument is interrupted by comparing to the sentence token ids in the relations.json file\n",
    "    slice_index = None\n",
    "    \n",
    "    for index, pair in enumerate(argument_tuples_list):\n",
    "        if pair == argument_tuples_list[-1]:\n",
    "            break\n",
    "\n",
    "        next_pair = argument_tuples_list[index + 1]\n",
    "\n",
    "\n",
    "        if (next_pair[0] - pair[0]) > 1:\n",
    "            slice_index = index\n",
    "            \n",
    "            break\n",
    "            \n",
    "        else:\n",
    "            slice_index = None\n",
    "    \n",
    "    argument_tokens_as_in_parsing = []\n",
    "    \n",
    "    for st_id, token in argument_tuples_list:\n",
    "        \n",
    "        argument_tokens_as_in_parsing.append(token)  \n",
    "    # Get the argument as either 1 string in a list (if it is not interrupted) or as 2 strings in a list\n",
    "    # if it is interrupted\n",
    "\n",
    "    if slice_index:\n",
    "        arg1_1 = ' '.join(argument_tokens_as_in_parsing[:slice_index + 1])\n",
    "\n",
    "        arg1_2 = ' '.join(argument_tokens_as_in_parsing[slice_index + 1:])\n",
    "\n",
    "        arg_str_list = [arg1_1, arg1_2]\n",
    "    else:\n",
    "\n",
    "        arg_str_list = [' '.join(argument_tokens_as_in_parsing)]\n",
    "    \n",
    "    \n",
    "    return arg_str_list\n",
    "\n",
    "get_arg_list(my_tuple_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Map arguments to constituents\n",
    "\n",
    "def constituent_structure(relation, arg):\n",
    "    \"\"\"\"\"\"\n",
    "\n",
    "    # Dicts with argument as key and constituent as value (tuple: (label, constituent))\n",
    "    arg_is_const = defaultdict(list)\n",
    "    const_part_arg = defaultdict(list)\n",
    "    \n",
    "    \n",
    "    \n",
    "    rel_id, doc_id, sentence_id_arg, arg_str, sentence_token_id_relations = get_data(relation, arg)\n",
    "    \n",
    "    arg_tuples_list = my_tuple_list = parse_sentence_token_id(doc_id, sentence_id_arg, sentence_token_id_relations)\n",
    "    \n",
    "    arg_list = get_arg_list(arg_tuples_list)\n",
    "\n",
    "\n",
    "    sentence = parses[doc_id]['sentences'][sentence_id_arg]['parsetree']\n",
    "\n",
    "    const_list = get_subtrees(sentence)\n",
    "\n",
    "\n",
    "    for argument in arg_list:\n",
    "        argument_list = argument.split()\n",
    "\n",
    "\n",
    "\n",
    "        for constituent in const_list:\n",
    "\n",
    "            label, const = constituent\n",
    "            c_list = const.split()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "            if const == argument:\n",
    "\n",
    "                arg_is_const[argument].append((label, const))\n",
    "                \n",
    "                break\n",
    "                \n",
    "            elif (const in argument) and (len(c_list) > 2):\n",
    "                const_part_arg[argument].append((label, const))\n",
    "                \n",
    "            \n",
    "            elif (const in argument) and (len(c_list) <= 2):\n",
    "                if const in argument_list:\n",
    "                    const_part_arg[argument].append((label, const))\n",
    "                \n",
    "    return  arg_is_const, const_part_arg, rel_id, doc_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict1: defaultdict(<class 'list'>, {})\n",
      "dict2: defaultdict(<class 'list'>, {'or receive cash from the funds': [('IN', 'from'), ('DT', 'the'), ('NNS', 'funds'), ('NNS', 'funds'), ('CC', 'or'), ('VP', 'receive cash from the funds'), ('VB', 'receive'), ('NP', 'cash from the funds'), ('NP', 'cash'), ('NN', 'cash'), ('PP', 'from the funds'), ('IN', 'from'), ('DT', 'the'), ('NNS', 'funds')]})\n",
      "yes\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "my_rel = relations_ss[0]\n",
    "\n",
    "dict1, dict2, id1, id2 = constituent_structure(my_rel, 'Arg1')\n",
    "\n",
    "\n",
    "print('dict1:',dict1)\n",
    "print('dict2:',dict2)\n",
    "\n",
    "if dict2:\n",
    "    print('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# WE have two dictionaries now, one containing arguments that match constituents and one of arguments that do \n",
    "# not match constituents. For the latter, we want to get the constituents that make up the argument.\n",
    "\n",
    "def argument_structure(relation, arg):\n",
    "    \"\"\"\"\"\"\n",
    "   \n",
    "        \n",
    "\n",
    "    arg_is_const_dict, dict4, rel_id, doc_id = constituent_structure(relation, arg)\n",
    "\n",
    "\n",
    "    # For all other cases, find the biggest constituents that make up the argument: \n",
    "    arg_const_dict = defaultdict(list)\n",
    "\n",
    "    # List of all arguments in the dictionary (interrupted arguments are two seperate keys)\n",
    "    # Make a list of all keys\n",
    "    arg = list(dict4.keys())\n",
    "\n",
    "\n",
    "    # for some reason, there was an empty list in one case (which makes the program break down)\n",
    "    if arg:\n",
    "    \n",
    "        # The argument is in a list now, but we need the string (there can only be one item in the list)\n",
    "        arg = arg[0]\n",
    "\n",
    "        # Print the argument:\n",
    "        print('\\n\\nthe argument: ',arg)\n",
    "\n",
    "        # Make a seperate list of all constituents (the values of each argument in the original dict)\n",
    "        const_list = list(dict4.values())\n",
    "        \n",
    "\n",
    "        # Print arguments to look at them:\n",
    "        #pprint.pprint(const_list)\n",
    "\n",
    "        # Make an empty list that will contain the lengths of constituents, the constituents and their label\n",
    "        const_len_list = []\n",
    "        # Again ,the constituents are lists, but there can only be one constituent per list (const_list[0])\n",
    "        for const in const_list[0]:\n",
    "            # the dict values were tuples of the constituent label and the constituent, unpack them:\n",
    "            label, c = const\n",
    "            \n",
    "            # In order to get the lenght of the constituent, make a list:\n",
    "            c_list = c.split()\n",
    "            # Fill the list with the constituent length, the constituent and its label\n",
    "            const_len_list.append((len(c_list), c, label))\n",
    "\n",
    "        # Sort the list so the longest constituents will be on top\n",
    "        sorted_const_len_list = sorted(const_len_list, reverse = True)\n",
    "\n",
    "        # Start a list of the longest constituets (there might be two of the same length or the \n",
    "        # same analyzed as two different kinds of phrases (e.g. sentence and verb phrase))\n",
    "        longest_consts = []\n",
    "\n",
    "        # Append the longest ones to the list:\n",
    "        for const in sorted_const_len_list:\n",
    "            # Unpack tuple in length, constituent and label:\n",
    "            length, c, l = const\n",
    "\n",
    "            # Same constituents analyzed as different phrase structures should be appended:\n",
    "            if (sorted_const_len_list[0][0] == length) and (sorted_const_len_list[0][1] == c) :\n",
    "\n",
    "                longest_consts.append(const)\n",
    "\n",
    "\n",
    "        # Print the list to see how many there are:\n",
    "        print('the longest constituent: ',longest_consts)\n",
    "        \n",
    "        \n",
    "\n",
    "        # Add longest constituent to arg_const_dict:\n",
    "        arg_const_dict[arg].append((longest_consts[0][2],longest_consts[0][1]) )\n",
    "\n",
    "        # Remove the longest constituents from the list of constituents that make up the argument \n",
    "        # (simply take the first one, as the label doesn't matter here)\n",
    "\n",
    "        rest = arg.replace(longest_consts[0][1], '')\n",
    "\n",
    "        # Print the rest of the argument:\n",
    "        print('my rest', rest)\n",
    "\n",
    "\n",
    "        # Loop through all constituents that make up the argument (sorted according to size):\n",
    "        for const in sorted_const_len_list:\n",
    "            # Position 0 is length, position 1 is the constituent and 2 the label\n",
    "            l = const[2] # label\n",
    "            c = const[1].strip() # constituent string\n",
    "            rest = rest.strip()\n",
    "\n",
    "\n",
    "            # If the constituent matches the rest of the agument, we have found the two constituents \n",
    "            # making up the argument and can leave the loop)\n",
    "            if c == rest:\n",
    "                print('constituent matches rest: ',l, c)\n",
    "                arg_const_dict[arg].append((l, c))\n",
    "                break\n",
    "\n",
    "            # If this is not the case, but the constituent is part of the rest of the argument, \n",
    "            # we have found another constituent and want to remove it from the rest. Replacing it with\n",
    "            # an empty character is a problem, because if we have a constituent like 'a', all as will \n",
    "            # be removed from the string (resulting in funny rest strings like 'tody' instead of today). \n",
    "            # Therefore, I use two characters as a limit for replacing characters with ''. In cases in \n",
    "            # which constituents are only two characters long, I first split up the rest and make sure\n",
    "            # to only exclude the entire token (and not characters within tokens). Then I join the rest \n",
    "            # again. \n",
    "\n",
    "            elif c in rest:\n",
    "\n",
    "                rest_list = rest.split()\n",
    "\n",
    "\n",
    "\n",
    "                if len(c) > 2:\n",
    "\n",
    "                    rest = rest.replace(c, '').strip()\n",
    "                    print('another constituent', l, c)\n",
    "                    arg_const_dict[arg].append((l, c))\n",
    "\n",
    "                elif (len(c) <= 2) and (c in rest_list):\n",
    "                    rest_list.remove(c)\n",
    "                    rest = ' '.join(rest_list)\n",
    "                    print('another constituent', l, c)\n",
    "                    arg_const_dict[arg].append((l,c))\n",
    "\n",
    "    return arg_is_const_dict, arg_const_dict, rel_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(defaultdict(list,\n",
       "             {'He commissions and splendidly interprets fearsome contemporary scores and does some conducting': [('S',\n",
       "                'He commissions and splendidly interprets fearsome contemporary scores and does some conducting')]}),\n",
       " defaultdict(list, {}),\n",
       " 3186)"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test:\n",
    "my_rel = relations_ss[3]\n",
    "argument_structure(my_rel, 'Arg1')\n",
    "#argument_structure(my_rel, 'Arg2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "the argument:  or receive cash from the funds\n",
      "the longest constituent:  [(5, 'receive cash from the funds', 'VP')]\n",
      "my rest or \n",
      "constituent matches rest:  CC or\n",
      "\n",
      "\n",
      "the argument:  Solo woodwind players have to be creative\n",
      "the longest constituent:  [(3, 'Solo woodwind players', 'NP')]\n",
      "my rest  have to be creative\n",
      "another constituent TO to\n",
      "another constituent VBP have\n",
      "another constituent JJ creative\n",
      "constituent matches rest:  VB be\n",
      "\n",
      "\n",
      "the argument:  Solo woodwind players have to be creative if they want to work a lot\n",
      "the longest constituent:  [(10, 'to be creative if they want to work a lot', 'VP'), (10, 'to be creative if they want to work a lot', 'S')]\n",
      "my rest Solo woodwind players have \n",
      "another constituent NP Solo woodwind players\n",
      "constituent matches rest:  VBP have\n",
      "\n",
      "\n",
      "the argument:  Today , the pixie-like clarinetist has mostly dropped the missionary work\n",
      "the longest constituent:  [(3, 'the pixie-like clarinetist', 'NP')]\n",
      "my rest Today ,  has mostly dropped the missionary work\n",
      "another constituent NP the missionary work\n",
      "another constituent RB mostly\n",
      "another constituent VBZ has\n",
      "another constituent VBN dropped\n",
      "another constituent NP Today\n",
      "constituent matches rest:  , ,\n"
     ]
    }
   ],
   "source": [
    "for rel in relations_ss[:5]:\n",
    "    #print(n)\n",
    "    dict1, dict2, rel_id = argument_structure(rel, 'Arg1')\n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
