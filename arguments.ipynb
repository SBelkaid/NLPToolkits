{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I worked with this notbeook inside the development data folder, so if you would like to run the code, download it and put it in the right folder (or change the paths in the notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "import json\n",
    "import codecs\n",
    "from nltk.tree import Tree\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Specify path to data to be analyzed (I entered the path to the tutorial data for now, so that the code runs)\n",
    "\n",
    "with codecs.open ('tutorial/conll16st-en-01-12-16-trial/relations.json', 'r', encoding = 'utf-8') as pdtb_file:\n",
    "    \n",
    "\n",
    "    # Assign all relations (a list) to a variable\n",
    "    relations = [json.loads(x) for x in pdtb_file];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "{'Arg1': {'CharacterSpanList': [[783, 877]],\n",
      "          'RawText': 'Several big securities firms backed off from program '\n",
      "                     'trading a few months after the 1987 crash',\n",
      "          'TokenList': [[783, 790, 143, 4, 0],\n",
      "                        [791, 794, 144, 4, 1],\n",
      "                        [795, 805, 145, 4, 2],\n",
      "                        [806, 811, 146, 4, 3],\n",
      "                        [812, 818, 147, 4, 4],\n",
      "                        [819, 822, 148, 4, 5],\n",
      "                        [823, 827, 149, 4, 6],\n",
      "                        [828, 835, 150, 4, 7],\n",
      "                        [836, 843, 151, 4, 8],\n",
      "                        [844, 845, 152, 4, 9],\n",
      "                        [846, 849, 153, 4, 10],\n",
      "                        [850, 856, 154, 4, 11],\n",
      "                        [857, 862, 155, 4, 12],\n",
      "                        [863, 866, 156, 4, 13],\n",
      "                        [867, 871, 157, 4, 14],\n",
      "                        [872, 877, 158, 4, 15]]},\n",
      " 'Arg2': {'CharacterSpanList': [[883, 957]],\n",
      "          'RawText': 'most of them, led by Morgan Stanley & Co., moved back in '\n",
      "                     'earlier this year',\n",
      "          'TokenList': [[883, 887, 161, 5, 1],\n",
      "                        [888, 890, 162, 5, 2],\n",
      "                        [891, 895, 163, 5, 3],\n",
      "                        [895, 896, 164, 5, 4],\n",
      "                        [897, 900, 165, 5, 5],\n",
      "                        [901, 903, 166, 5, 6],\n",
      "                        [904, 910, 167, 5, 7],\n",
      "                        [911, 918, 168, 5, 8],\n",
      "                        [919, 920, 169, 5, 9],\n",
      "                        [921, 924, 170, 5, 10],\n",
      "                        [924, 925, 171, 5, 11],\n",
      "                        [926, 931, 172, 5, 12],\n",
      "                        [932, 936, 173, 5, 13],\n",
      "                        [937, 939, 174, 5, 14],\n",
      "                        [940, 947, 175, 5, 15],\n",
      "                        [948, 952, 176, 5, 16],\n",
      "                        [953, 957, 177, 5, 17]]},\n",
      " 'Connective': {'CharacterSpanList': [[879, 882]],\n",
      "                'RawText': 'But',\n",
      "                'TokenList': [[879, 882, 160, 5, 0]]},\n",
      " 'DocID': 'wsj_1000',\n",
      " 'ID': 14878,\n",
      " 'Sense': ['Comparison.Contrast'],\n",
      " 'Type': 'Explicit'}\n"
     ]
    }
   ],
   "source": [
    "# Loop through relations and select only explicit relations:\n",
    "\n",
    "# List for explicit relations:\n",
    "\n",
    "relations_explicit = []\n",
    "\n",
    "for relation in relations:\n",
    "    if relation['Type'] == 'Explicit':\n",
    "        relations_explicit.append(relation)\n",
    "        \n",
    "print(len(relations_explicit))\n",
    "pprint.pprint(relations_explicit[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# Split in two lists according to arg1 in same sentence and arg1 in previous sentence:\n",
    "\n",
    "relations_ss = []\n",
    "relations_ps = []\n",
    "relations_other = []\n",
    "\n",
    "for relation in relations_explicit:\n",
    "    sentence_id_arg1 = relation['Arg1']['TokenList'][0][3]\n",
    "    sentence_id_connective = relation['Connective']['TokenList'][0][3]\n",
    "    sentence_id_arg2 = relation['Arg2']['TokenList'][0][3]\n",
    "    \n",
    "    if sentence_id_arg1 == sentence_id_connective == sentence_id_arg2:\n",
    "        relations_ss.append(relation)\n",
    "    elif int(sentence_id_arg1) == int(sentence_id_connective) - 1 == int(sentence_id_arg2) -1:\n",
    "        relations_ps.append(relation)\n",
    "    else: \n",
    "        relations_other.append(relation)\n",
    "print(len(relations_ps))\n",
    "print(len(relations_ss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Open parse file:\n",
    "\n",
    "with codecs.open('tutorial/conll16st-en-01-12-16-trial/parses.json', 'r', encoding = 'utf8') as parse_file:\n",
    "    parses = json.load(parse_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We would stop index arbitrage when the market is under stress\n",
      "We would stop index arbitrage when the market is under stress\n",
      "rtc's! Work didn t doesn t $50\n"
     ]
    }
   ],
   "source": [
    "# NOT NEEDED \n",
    "\n",
    "def normalize_str(text):\n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    text = text.lstrip('(')\n",
    "    \n",
    "    text = text.rstrip(')')\n",
    "    text_list = text.split()\n",
    "    \n",
    "    clean_text_list = []\n",
    "    \n",
    "    for word in text_list:\n",
    "        word = word.strip()\n",
    "        clean_text_list.append(word)\n",
    "    \n",
    "    clean_text = ' '.join(clean_text_list)\n",
    "    \n",
    "    return(clean_text)\n",
    "\n",
    "my_rel = relations_ss[7]\n",
    "\n",
    "arg1_str = my_rel['Arg1']['RawText']\n",
    "print(arg1_str)\n",
    "print(normalize_str(arg1_str))\n",
    "print(normalize_str(\"(rtc's! Work\\n didn t doesn t $50\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_const_type(constituent_list, constituent_name):\n",
    "    \"\"\"\n",
    "    Input: set of constituents, name of a constituent type (str)\n",
    "    Output: list of constituents of the selected type \n",
    "    \"\"\"\n",
    "    const_type_list = []\n",
    "    \n",
    "    for constituent in constituent_list:\n",
    "        \n",
    "        \n",
    "        if constituent.startswith('('+constituent_name):\n",
    "            const_type_list.append(constituent)\n",
    "    \n",
    "    return const_type_list\n",
    "            \n",
    "# E.g. get clauses:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('S',\n",
      "  \"They said , ` Too bad , ' so we finally said we 're not going to do \"\n",
      "  \"business with them . ''\"),\n",
      " ('S', 'They said , ` Too bad'),\n",
      " ('NP', 'They'),\n",
      " ('PRP', 'They'),\n",
      " ('VP', 'said , ` Too bad'),\n",
      " ('VBD', 'said'),\n",
      " (',', ','),\n",
      " ('ADJP', 'Too bad'),\n",
      " ('RB', 'Too'),\n",
      " ('JJ', 'bad'),\n",
      " (',', ','),\n",
      " (\"''\", \"'\"),\n",
      " ('IN', 'so'),\n",
      " ('S', \"we finally said we 're not going to do business with them\"),\n",
      " ('NP', 'we'),\n",
      " ('PRP', 'we'),\n",
      " ('ADVP', 'finally'),\n",
      " ('RB', 'finally'),\n",
      " ('VP', \"said we 're not going to do business with them\"),\n",
      " ('VBD', 'said'),\n",
      " ('SBAR', \"we 're not going to do business with them\"),\n",
      " ('S', \"we 're not going to do business with them\"),\n",
      " ('NP', 'we'),\n",
      " ('PRP', 'we'),\n",
      " ('VP', \"'re not going to do business with them\"),\n",
      " ('VBP', \"'re\"),\n",
      " ('RB', 'not'),\n",
      " ('VP', 'going to do business with them'),\n",
      " ('VBG', 'going'),\n",
      " ('S', 'to do business with them'),\n",
      " ('VP', 'to do business with them'),\n",
      " ('TO', 'to'),\n",
      " ('VP', 'do business with them'),\n",
      " ('VB', 'do'),\n",
      " ('NP', 'business'),\n",
      " ('NN', 'business'),\n",
      " ('PP', 'with them'),\n",
      " ('IN', 'with'),\n",
      " ('NP', 'them'),\n",
      " ('PRP', 'them'),\n",
      " ('.', '.'),\n",
      " (\"''\", \"''\")]\n"
     ]
    }
   ],
   "source": [
    "def get_subtrees(parsed_sentence, const_type = 0):\n",
    "    \"\"\"\"\"\"\n",
    "    tree_nltk = Tree.fromstring(parsed_sentence)\n",
    "    \n",
    "    tree_height = tree_nltk.height()\n",
    "    \n",
    "        \n",
    "    const_tuple_list = []\n",
    "\n",
    "    for s in tree_nltk.subtrees():\n",
    "\n",
    "        label = s.label()\n",
    "        \n",
    "        if (label != '``') and (label != ''):\n",
    "            \n",
    "            subtree_tokens_str = str(s.flatten())\n",
    "            #print(type(subtree_tokens_str))\n",
    "            clean_subtree_str = normalize_str(subtree_tokens_str).lstrip(label).lstrip(' ')\n",
    "\n",
    "\n",
    "\n",
    "            if const_type != 0:\n",
    "                if label == const_type:\n",
    "                    const_tuple_list.append((label, clean_subtree_str))\n",
    "\n",
    "            else:\n",
    "\n",
    "                const_tuple_list.append((label, clean_subtree_str))\n",
    "\n",
    "\n",
    "\n",
    "    return const_tuple_list\n",
    "\n",
    "doc_id = my_rel['DocID']\n",
    "sentence_id_arg1 = my_rel['Arg1']['TokenList'][0][3]\n",
    "parsed_sentence = parses[doc_id]['sentences'][sentence_id_arg1]['parsetree']  \n",
    "\n",
    "consts = get_subtrees(parsed_sentence)\n",
    "\n",
    "pprint.pprint(consts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wsj_1000 [0, 1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "def get_data(relation, arg):\n",
    "    \n",
    "    \"\"\"returns DocID, sentenceID, Argument raw text and sentence_token_id in relations.json\"\"\"\n",
    "    \n",
    "    doc_id = relation['DocID']\n",
    "    sentence_id_arg = relation[arg]['TokenList'][0][3]\n",
    "    arg_str = relation[arg]['RawText']   \n",
    "    \n",
    "    sentence_token_id_relations = []\n",
    "    \n",
    "    relations_token_list = relation[arg]['TokenList']\n",
    "    \n",
    "    for line in relations_token_list:\n",
    "        sentence_token_id = line[4]\n",
    "        sentence_token_id_relations.append(sentence_token_id)\n",
    "    \n",
    "    return doc_id, sentence_id_arg, arg_str, sentence_token_id_relations\n",
    "\n",
    "my_rel = relations_ss[3]\n",
    "my_id, my_sid, my_arg, my_st = get_data(my_rel, 'Arg1')\n",
    "print(my_id, my_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '``'), (1, 'The'), (2, 'impression'), (3, 'I'), (4, \"'ve\"), (5, 'got')]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_sentence_token_id(doc_id, sentence_id_arg, sentence_token_id_relations):\n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    argument_id_tuples_list = []\n",
    "    \n",
    "    parsed_words_list = parses[doc_id]['sentences'][sentence_id_arg]['words']\n",
    "    \n",
    "    for number, word in enumerate(parsed_words_list):\n",
    "        \n",
    "        if number in sentence_token_id_relations:\n",
    "        \n",
    "            argument_id_tuples_list.append((number, word[0]))\n",
    "        \n",
    "        \n",
    "    return argument_id_tuples_list\n",
    "\n",
    "parse_sentence_token_id('wsj_1000', 22, my_st)\n",
    "\n",
    "# Looks good, if you compare these ids to the sentence_token_ids taken from relations.json. \n",
    "# Covers the span of the argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"it scares natural buyers '' of stock\"]\n",
      "[\"Mr. Timbers explained he 's `` not totally convinced index arbitrage changes the overall level of the stock market\"]\n",
      "[\"they 're ruining the market\"]\n",
      "[\"we finally said we 're not going to do business with them\"]\n",
      "[\"they -LCB- the exchange -RCB- ca n't do it\"]\n",
      "[\"that all the exchange can do is `` slow down the process '' by using its circuit breakers and shock absorbers\"]\n",
      "['the market is under stress']\n",
      "['we have recently']\n",
      "['But it may become much more important']\n"
     ]
    }
   ],
   "source": [
    "def get_arg_list(relation, arg):\n",
    "    \n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    doc_id, sentence_id_arg, arg_str, sentence_token_id_relations = get_data(relation, arg)\n",
    "\n",
    "    \n",
    "    sentence_token_id_parse_tuples = parse_sentence_token_id(doc_id, sentence_id_arg, sentence_token_id_relations)\n",
    "\n",
    "    # check if argument is interrupted by comparing to the sentence token ids in the relations.json file\n",
    "    \n",
    "    for index, pair in enumerate(sentence_token_id_parse_tuples):\n",
    "        if pair == sentence_token_id_parse_tuples[-1]:\n",
    "            break\n",
    "\n",
    "        next_pair = sentence_token_id_parse_tuples[index + 1]\n",
    "\n",
    "\n",
    "        if (next_pair[0] - pair[0]) > 1:\n",
    "            slice_index = index\n",
    "            \n",
    "            break\n",
    "        else:\n",
    "            slice_index = None\n",
    "    \n",
    "    argument_tokens_as_in_parsing = []\n",
    "    \n",
    "    for st_id, token in sentence_token_id_parse_tuples:\n",
    "        \n",
    "        argument_tokens_as_in_parsing.append(token)  \n",
    "    # Get the argument as either 1 string in a list (if it is not interrupted) or as 2 strings in a list\n",
    "    # if it is interrupted\n",
    "\n",
    "    if slice_index:\n",
    "        arg1_1 = ' '.join(argument_tokens_as_in_parsing[:slice_index + 1])\n",
    "\n",
    "        arg1_2 = ' '.join(argument_tokens_as_in_parsing[slice_index + 1:])\n",
    "\n",
    "        arg_str_list = [arg1_1, arg1_2]\n",
    "    else:\n",
    "\n",
    "        arg_str_list = [' '.join(argument_tokens_as_in_parsing)]\n",
    "    \n",
    "    \n",
    "    return arg_str_list\n",
    "\n",
    "for relation in relations_ss:\n",
    "    print(get_arg_list(relation, 'Arg2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Map arguments to constituents\n",
    "\n",
    "def constituent_structure(relation, arg):\n",
    "    \"\"\"\"\"\"\n",
    "\n",
    "    # Dicts with argument as key and constituent as value (tuple: (label, constituent))\n",
    "    arg_is_const = defaultdict(list)\n",
    "    arg_starts_const = defaultdict(list)\n",
    "    arg_in_const = defaultdict(list)\n",
    "    const_part_arg = defaultdict(list)\n",
    "    \n",
    "    \n",
    "    arg_list = get_arg_list(relation, arg)\n",
    "    \n",
    "    doc_id, sentence_id_arg, arg_str, sentence_token_id_relations = get_data(relation, arg)\n",
    "\n",
    "\n",
    "    sentence = parses[doc_id]['sentences'][sentence_id_arg]['parsetree']\n",
    "\n",
    "    const_list = get_subtrees(sentence)\n",
    "\n",
    "\n",
    "    for argument in arg_list:\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        for constituent in const_list:\n",
    "\n",
    "            label, const = constituent\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "            # Constituents the argument is part of:\n",
    "\n",
    "            if const == argument:\n",
    "\n",
    "                arg_is_const[argument].append((label, const))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            elif const.startswith(argument):\n",
    "\n",
    "                arg_starts_const[argument].append((label, const))\n",
    "\n",
    "\n",
    "            elif argument in const:\n",
    "\n",
    "                arg_in_const[argument].append((label, const))\n",
    "\n",
    "\n",
    "\n",
    "            # Constituents that are part of the argument\n",
    "\n",
    "            if const in argument:\n",
    "                const_part_arg[argument].append((label, const))\n",
    "                \n",
    "    return arg_is_const, arg_starts_const, arg_in_const, const_part_arg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(defaultdict(list,\n",
       "             {\"Index arbitrage does n't work\": [('S',\n",
       "                \"Index arbitrage does n't work\")]}),\n",
       " defaultdict(list, {}),\n",
       " defaultdict(list,\n",
       "             {\"Index arbitrage does n't work\": [('S',\n",
       "                \"`` Index arbitrage does n't work , and it scares natural buyers '' of stock .\")]}),\n",
       " defaultdict(list,\n",
       "             {\"Index arbitrage does n't work\": [('S',\n",
       "                \"Index arbitrage does n't work\"),\n",
       "               ('NP', 'Index arbitrage'),\n",
       "               ('NN', 'Index'),\n",
       "               ('NN', 'arbitrage'),\n",
       "               ('VP', \"does n't work\"),\n",
       "               ('VBZ', 'does'),\n",
       "               ('RB', \"n't\"),\n",
       "               ('VP', 'work'),\n",
       "               ('VB', 'work'),\n",
       "               ('NP', 'it'),\n",
       "               ('PRP', 'it')]}))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constituent_structure(relations_ss[0], 'Arg1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
