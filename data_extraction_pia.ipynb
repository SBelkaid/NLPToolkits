{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "import json\n",
    "import codecs\n",
    "from nltk.tree import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'Arg1': {'CharacterSpanList': [[2493, 2517]],\n",
      "          'RawText': 'and told them to cool it',\n",
      "          'TokenList': [[2493, 2496, 465, 15, 8],\n",
      "                        [2497, 2501, 466, 15, 9],\n",
      "                        [2502, 2506, 467, 15, 10],\n",
      "                        [2507, 2509, 468, 15, 11],\n",
      "                        [2510, 2514, 469, 15, 12],\n",
      "                        [2515, 2517, 470, 15, 13]]},\n",
      " 'Arg2': {'CharacterSpanList': [[2526, 2552]],\n",
      "          'RawText': \"they're ruining the market\",\n",
      "          'TokenList': [[2526, 2530, 472, 15, 15],\n",
      "                        [2530, 2533, 473, 15, 16],\n",
      "                        [2534, 2541, 474, 15, 17],\n",
      "                        [2542, 2545, 475, 15, 18],\n",
      "                        [2546, 2552, 476, 15, 19]]},\n",
      " 'Connective': {'CharacterSpanList': [[2518, 2525]],\n",
      "                'RawText': 'because',\n",
      "                'TokenList': [[2518, 2525, 471, 15, 14]]},\n",
      " 'DocID': 'wsj_1000',\n",
      " 'ID': 14887,\n",
      " 'Sense': ['Contingency.Cause.Reason'],\n",
      " 'Type': 'Explicit'}\n"
     ]
    }
   ],
   "source": [
    "# View data structure: json\n",
    "    # List of dicts that can have dicts or lists as values\n",
    "\n",
    "with codecs.open ('tutorial/conll16st-en-01-12-16-trial/relations.json', 'r', encoding = 'utf-8') as pdtb_file:\n",
    "    \n",
    "\n",
    "    # Assign all relations (a list) to a variable\n",
    "    relations = [json.loads(x) for x in pdtb_file];\n",
    "    \n",
    "# Assign one relation to a variable\n",
    "example_relation = relations[10]\n",
    "\n",
    "\n",
    "\n",
    "# Dict\n",
    "print(type(example_relation))\n",
    "pprint.pprint(example_relation)\n",
    "\n",
    "# Assign part of a list element to a variable:\n",
    "# Dicts (arg1, arg2, connective):\n",
    "    # Keys: 'CharacterSpanList', 'RawText', TokenList', \n",
    "arg1 = example_relation['Arg1']\n",
    "arg2 = example_relation['Arg2']\n",
    "connective = example_relation['Connective']\n",
    "\n",
    "sentence_id_arg1 = arg1['TokenList'][0][3]\n",
    "sentence_id_arg2 = arg2['TokenList'][0][3]\n",
    "sentence_id_connective = connective['TokenList'][0][3]\n",
    "# We can easily see which relations have arg1 in the previous sentence and which in the same sentence by comparing\n",
    "# the sentence ids of arg1 and connective or arg2. \n",
    "\n",
    "#Other keys:\n",
    "\n",
    "DocID = example_relation['DocID']\n",
    "relationID = example_relation['ID']\n",
    "relationtype = example_relation['Type']\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# Open json file containing the parses and assign them to a variable:\n",
    "with codecs.open('tutorial/conll16st-en-01-12-16-trial/parses.json', 'r', encoding = 'utf8') as parse_file:\n",
    "    parses = json.load(parse_file)\n",
    "    \n",
    "# Explore data structure\n",
    "print(type(parses))\n",
    "\n",
    "# Take a look at the tructure:\n",
    "#pprint.pprint(list(parses.items())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( (S (NP (PRP We)) (VP (VBP 've) (VP (VP (VBN talked) (PP (TO to) (NP (NP (NNS proponents)) (PP (IN of) (NP (NN index) (NN arbitrage)))))) (CC and) (VP (VBD told) (NP (PRP them)) (S (VP (TO to) (VP (VB cool) (NP (PRP it)) (SBAR (IN because) (S (NP (PRP they)) (VP (VBP 're) (VP (VBG ruining) (NP (DT the) (NN market)))))))))))) (. .)) )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Work with example relation defined above\n",
    "\n",
    "# Print parse tree of example relations (which happens to be in one sentence):\n",
    "print(parses[DocID]['sentences'][sentence_id_arg1]['parsetree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'dict'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('We', 'PRP'),\n",
       " (\"'ve\", 'VBP'),\n",
       " ('talked', 'VBN'),\n",
       " ('to', 'TO'),\n",
       " ('proponents', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('index', 'NN'),\n",
       " ('arbitrage', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('told', 'VBD'),\n",
       " ('them', 'PRP'),\n",
       " ('to', 'TO'),\n",
       " ('cool', 'VB'),\n",
       " ('it', 'PRP'),\n",
       " ('because', 'IN'),\n",
       " ('they', 'PRP'),\n",
       " (\"'re\", 'VBP'),\n",
       " ('ruining', 'VBG'),\n",
       " ('the', 'DT'),\n",
       " ('market', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get words and POS \n",
    "\n",
    "example_sentence = parses[DocID]['sentences'][sentence_id_arg1]\n",
    "#pprint.pprint(example_sentence['words'])\n",
    "\n",
    "# Explore data structure\n",
    "print(type(example_sentence['words']))\n",
    "print(type(example_sentence['words'][0][1]))\n",
    "\n",
    "# Take a look at the data structure:\n",
    "#pprint.pprint(example_sentence['words'])\n",
    "    \n",
    "##################################\n",
    "\n",
    "# Function to get all tokens and POS tags of a sentence\n",
    "\n",
    "def get_pos(doc_id, sentence_id):\n",
    "    \n",
    "    \"\"\"\"\"\"\n",
    "    # Get  parse tree of sentence:\n",
    "    \n",
    "    parsed_sentence = parses[doc_id]['sentences'][sentence_id]\n",
    "    \n",
    "    sentence_pos_list = []\n",
    "    \n",
    "    for word in parsed_sentence['words']:\n",
    "        token = word[0]\n",
    "        pos = word[1]['PartOfSpeech']\n",
    "        \n",
    "        sentence_pos_list.append((token, pos))\n",
    "        \n",
    "    return sentence_pos_list\n",
    "\n",
    "\n",
    "# We can then add this list of tuples to a dict with sentence ids as keys and lists of tuples (token, pos) \n",
    "# as values\n",
    "get_pos(DocID, sentence_id_arg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(SBAR\\n'\n",
      " '  (IN because)\\n'\n",
      " '  (S\\n'\n",
      " '    (NP (PRP they))\\n'\n",
      " \"    (VP (VBP 're) (VP (VBG ruining) (NP (DT the) (NN market))))))\"]\n"
     ]
    }
   ],
   "source": [
    "# Explore parse tree\n",
    "\n",
    "#example_tree = parses[DocID]['sentences'][sentence_id_arg1]['parsetree']\n",
    "\n",
    "#print(example_tree)\n",
    "#print(type(example_tree))\n",
    "\n",
    "# Load string as tree:\n",
    "\n",
    "#tree_nltk = Tree.fromstring(example_tree)\n",
    "#print(type(tree_nltk))\n",
    "\n",
    "# Parsing the tree:\n",
    "\n",
    "#print(tree_nltk.flatten())\n",
    "\n",
    "# Draw your tree :-) \n",
    "#tree_nltk.draw() \n",
    "#tree_nltk.freeze()\n",
    "\n",
    "# Height of the tree\n",
    "\n",
    "\n",
    "# As far as I understand, the filter function in the subtrees function shows only subtrees (i.e. consituents)\n",
    "# of the selected depth. We want all possible constituents (maybe not the smallest ones consisting of only one\n",
    "# word, but here I haven't excluded them), so I loop through all possible depths (the deepest one is the absolute \n",
    "# height of the tree).\n",
    "\n",
    "\n",
    "##############################\n",
    "\n",
    "# Funcktion to extract all constituents:\n",
    "\n",
    "def get_constituents (doc_id, sentence_id):\n",
    "    \"\"\"\n",
    "    Input: document ID (str), sentence ID (str)\n",
    "    Output: a set of constituents of the sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    tree = parses[doc_id]['sentences'][sentence_id]['parsetree']\n",
    "    \n",
    "    tree_nltk = Tree.fromstring(tree)\n",
    "    \n",
    "    tree_height = tree_nltk.height()\n",
    "    \n",
    "    constituent_list = []\n",
    "    constituent_set = set()\n",
    "    \n",
    "    for n in range(tree_height):\n",
    "        for s in tree_nltk.subtrees(lambda t: t.height() == n):\n",
    "            constituent_str = str(s)\n",
    "            constituent_set.add(constituent_str)\n",
    "            \n",
    "    \n",
    "\n",
    "    return constituent_set\n",
    "\n",
    "# Look at individual constituents:\n",
    "constituents = list(get_constituents(DocID, sentence_id_arg1))\n",
    "\n",
    "#print(constituents[11])\n",
    "\n",
    "\n",
    "# Get only constituents of a particular type:\n",
    "\n",
    "def get_const_type(constituent_list, constituent_name):\n",
    "    \"\"\"\"\"\"\n",
    "    const_type_list = []\n",
    "    \n",
    "    for constituent in constituent_list:\n",
    "        \n",
    "        \n",
    "        if constituent.startswith('('+constituent_name):\n",
    "            const_type_list.append(constituent)\n",
    "    \n",
    "    return const_type_list\n",
    "            \n",
    "# E.g. get clauses:   \n",
    "pprint.pprint(get_const_type(constituents, 'SBAR'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data Structure:\n",
    "\n",
    "Dictionary with unique ID\n",
    "    For each relation:\n",
    "        Connector\n",
    "        Form\n",
    "        PoS\n",
    "        Constituents\n",
    "    Previous sentence\n",
    "        Tokens\n",
    "        Lemma\n",
    "        PoS\n",
    "        Constituents ([CAT, [w1, .. ,wn]])\n",
    "        Dependency\n",
    "    Current sentence\n",
    "        Tokens\n",
    "        Lemma\n",
    "        PoS\n",
    "        Constituents\n",
    "        Dependency\n",
    "    Gold Arg1\n",
    "    Gold Arg2\n",
    "    Relation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
