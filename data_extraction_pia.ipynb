{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "import json\n",
    "import codecs\n",
    "from nltk.tree import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'Arg1': {'CharacterSpanList': [[2493, 2517]],\n",
      "          'RawText': 'and told them to cool it',\n",
      "          'TokenList': [[2493, 2496, 465, 15, 8],\n",
      "                        [2497, 2501, 466, 15, 9],\n",
      "                        [2502, 2506, 467, 15, 10],\n",
      "                        [2507, 2509, 468, 15, 11],\n",
      "                        [2510, 2514, 469, 15, 12],\n",
      "                        [2515, 2517, 470, 15, 13]]},\n",
      " 'Arg2': {'CharacterSpanList': [[2526, 2552]],\n",
      "          'RawText': \"they're ruining the market\",\n",
      "          'TokenList': [[2526, 2530, 472, 15, 15],\n",
      "                        [2530, 2533, 473, 15, 16],\n",
      "                        [2534, 2541, 474, 15, 17],\n",
      "                        [2542, 2545, 475, 15, 18],\n",
      "                        [2546, 2552, 476, 15, 19]]},\n",
      " 'Connective': {'CharacterSpanList': [[2518, 2525]],\n",
      "                'RawText': 'because',\n",
      "                'TokenList': [[2518, 2525, 471, 15, 14]]},\n",
      " 'DocID': 'wsj_1000',\n",
      " 'ID': 14887,\n",
      " 'Sense': ['Contingency.Cause.Reason'],\n",
      " 'Type': 'Explicit'}\n",
      "{'CharacterSpanList': [[2493, 2517]],\n",
      " 'RawText': 'and told them to cool it',\n",
      " 'TokenList': [[2493, 2496, 465, 15, 8],\n",
      "               [2497, 2501, 466, 15, 9],\n",
      "               [2502, 2506, 467, 15, 10],\n",
      "               [2507, 2509, 468, 15, 11],\n",
      "               [2510, 2514, 469, 15, 12],\n",
      "               [2515, 2517, 470, 15, 13]]}\n"
     ]
    }
   ],
   "source": [
    "# View data structure: json\n",
    "    # List of dicts that can have dicts or lists as values\n",
    "\n",
    "with codecs.open ('tutorial/conll16st-en-01-12-16-trial/relations.json', 'r', encoding = 'utf-8') as pdtb_file:\n",
    "    \n",
    "\n",
    "    # Assign all relations (a list) to a variable\n",
    "    relations = [json.loads(x) for x in pdtb_file];\n",
    "    \n",
    "# Assign one relation to a variable\n",
    "example_relation = relations[10]\n",
    "\n",
    "\n",
    "\n",
    "# Dict\n",
    "print(type(example_relation))\n",
    "pprint.pprint(example_relation)\n",
    "\n",
    "# Assign part of a list element to a variable:\n",
    "# Dicts (arg1, arg2, connective):\n",
    "    # Keys: 'CharacterSpanList', 'RawText', TokenList', \n",
    "arg1 = example_relation['Arg1']\n",
    "arg2 = example_relation['Arg2']\n",
    "connective = example_relation['Connective']\n",
    "\n",
    "sentence_id_arg1 = arg1['TokenList'][0][3]\n",
    "sentence_id_arg2 = arg2['TokenList'][0][3]\n",
    "sentence_id_connective = connective['TokenList'][0][3]\n",
    "# We can easily see which relations have arg1 in the previous sentence and which in the same sentence by comparing\n",
    "# the sentence ids of arg1 and connective or arg2. \n",
    "\n",
    "   \n",
    "\n",
    "#Other keys:\n",
    "\n",
    "DocID = example_relation['DocID']\n",
    "relationID = example_relation['ID']\n",
    "relationtype = example_relation['Type']\n",
    "\n",
    "pprint.pprint(arg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "( (S (NP (PRP We)) (VP (VBP 've) (VP (VP (VBN talked) (PP (TO to) (NP (NP (NNS proponents)) (PP (IN of) (NP (NN index) (NN arbitrage)))))) (CC and) (VP (VBD told) (NP (PRP them)) (S (VP (TO to) (VP (VB cool) (NP (PRP it)) (SBAR (IN because) (S (NP (PRP they)) (VP (VBP 're) (VP (VBG ruining) (NP (DT the) (NN market)))))))))))) (. .)) )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Open json file containing the parses and assign them to a variable:\n",
    "with codecs.open('tutorial/conll16st-en-01-12-16-trial/parses.json', 'r', encoding = 'utf8') as parse_file:\n",
    "    parses = json.load(parse_file)\n",
    "    \n",
    "# Explore data structure\n",
    "print(type(parses))\n",
    "\n",
    "# Take a look at the tructure:\n",
    "#pprint.pprint(list(parses.items())[0])\n",
    "\n",
    "#Work with example relation defined above\n",
    "\n",
    "# Print parse tree of example relations (which happens to be in one sentence):\n",
    "print(parses[DocID]['sentences'][sentence_id_arg1]['parsetree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'dict'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('We', 'PRP'),\n",
       " (\"'ve\", 'VBP'),\n",
       " ('talked', 'VBN'),\n",
       " ('to', 'TO'),\n",
       " ('proponents', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('index', 'NN'),\n",
       " ('arbitrage', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('told', 'VBD'),\n",
       " ('them', 'PRP'),\n",
       " ('to', 'TO'),\n",
       " ('cool', 'VB'),\n",
       " ('it', 'PRP'),\n",
       " ('because', 'IN'),\n",
       " ('they', 'PRP'),\n",
       " (\"'re\", 'VBP'),\n",
       " ('ruining', 'VBG'),\n",
       " ('the', 'DT'),\n",
       " ('market', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get words and POS \n",
    "\n",
    "example_sentence = parses[DocID]['sentences'][sentence_id_arg1]\n",
    "#pprint.pprint(example_sentence['words'])\n",
    "\n",
    "# Explore data structure\n",
    "print(type(example_sentence['words']))\n",
    "print(type(example_sentence['words'][0][1]))\n",
    "\n",
    "# Take a look at the data structure:\n",
    "#pprint.pprint(example_sentence['words'])\n",
    "    \n",
    "##################################\n",
    "\n",
    "# Function to get all tokens and POS tags of a sentence\n",
    "\n",
    "def get_pos(doc_id, sentence_id):\n",
    "    \n",
    "    \"\"\"\"\"\"\n",
    "    # Get  parse tree of sentence:\n",
    "    \n",
    "    parsed_sentence = parses[doc_id]['sentences'][sentence_id]\n",
    "    \n",
    "    sentence_pos_list = []\n",
    "    \n",
    "    for word in parsed_sentence['words']:\n",
    "        token = word[0]\n",
    "        pos = word[1]['PartOfSpeech']\n",
    "    \n",
    "    \n",
    "        sentence_pos_list.append((token, pos))\n",
    "    \n",
    "        \n",
    "    return sentence_pos_list\n",
    "\n",
    "\n",
    "# We can then add this list of tuples to a dict with sentence ids as keys and lists of tuples (token, pos) \n",
    "# as values\n",
    "get_pos(DocID, sentence_id_arg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2518\n",
      "['IN', 'because']\n"
     ]
    }
   ],
   "source": [
    "# Get connector pos\n",
    "# Make sure to select pos of the word that acts as connector\n",
    "# Get connective offset (no token ids given in the parsing data, maybe it would be smart to add them ourselves?)\n",
    "\n",
    "connective_offset = connective['TokenList'][0][0]\n",
    "print(connective_offset)\n",
    "\n",
    "\n",
    "            \n",
    "def get_pos_connective(doc_id, sentence_id, connective_offset):\n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    sentence = parses[DocID]['sentences'][sentence_id_arg1]\n",
    "    \n",
    "    for word in example_sentence['words']:\n",
    "        if word[1]['CharacterOffsetBegin'] == connective_offset:\n",
    "            \n",
    "            return [word[1]['PartOfSpeech'], word[0]]\n",
    "\n",
    "target_connective_pos = get_pos_connective(DocID, sentence_id_arg1, connective_offset)\n",
    "print(target_connective_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(SBAR\\n'\n",
      " '  (IN because)\\n'\n",
      " '  (S\\n'\n",
      " '    (NP (PRP they))\\n'\n",
      " \"    (VP (VBP 're) (VP (VBG ruining) (NP (DT the) (NN market))))))\"]\n"
     ]
    }
   ],
   "source": [
    "# CONSTITUENTS\n",
    "\n",
    "# Explore parse tree\n",
    "\n",
    "#example_tree = parses[DocID]['sentences'][sentence_id_arg1]['parsetree']\n",
    "\n",
    "#print(example_tree)\n",
    "#print(type(example_tree))\n",
    "\n",
    "# Load string as tree:\n",
    "\n",
    "#tree_nltk = Tree.fromstring(example_tree)\n",
    "#print(type(tree_nltk))\n",
    "\n",
    "# Parsing the tree:\n",
    "\n",
    "#print(tree_nltk.flatten())\n",
    "\n",
    "# Draw your tree :-) \n",
    "#tree_nltk.draw() \n",
    "#tree_nltk.freeze()\n",
    "\n",
    "# Height of the tree\n",
    "\n",
    "\n",
    "# As far as I understand, the filter function in the subtrees function shows only subtrees (i.e. consituents)\n",
    "# of the selected depth. We want all possible constituents (maybe not the smallest ones consisting of only one\n",
    "# word, but here I haven't excluded them), so I loop through all possible depths (the deepest one is the absolute \n",
    "# height of the tree).\n",
    "\n",
    "\n",
    "##############################\n",
    "\n",
    "# Funcktion to extract all constituents:\n",
    "\n",
    "def get_constituents (doc_id, sentence_id):\n",
    "    \"\"\"\n",
    "    Input: document ID (str), sentence ID (str)\n",
    "    Output: a set of constituents of the sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    tree = parses[doc_id]['sentences'][sentence_id]['parsetree']\n",
    "    \n",
    "    tree_nltk = Tree.fromstring(tree)\n",
    "    \n",
    "    tree_height = tree_nltk.height()\n",
    "    \n",
    "    constituent_list = []\n",
    "    constituent_set = set()\n",
    "    \n",
    "    for n in range(tree_height):\n",
    "        for s in tree_nltk.subtrees(lambda t: t.height() == n):\n",
    "            constituent_str = str(s)\n",
    "            constituent_set.add(constituent_str)\n",
    "            \n",
    "    \n",
    "\n",
    "    return constituent_set\n",
    "\n",
    "# Look at individual constituents:\n",
    "const_set = get_constituents(DocID, sentence_id_arg1)\n",
    "constituents = list(get_constituents(DocID, sentence_id_arg1))\n",
    "\n",
    "#print(constituents[11])\n",
    "\n",
    "\n",
    "# Get only constituents of a particular type:\n",
    "\n",
    "def get_const_type(constituent_set, constituent_name):\n",
    "    \"\"\"\n",
    "    Input: set of constituents, name of a constituent type (str)\n",
    "    Output: list of constituents of the selected type \n",
    "    \"\"\"\n",
    "    const_type_list = []\n",
    "    \n",
    "    for constituent in constituent_set:\n",
    "        \n",
    "        \n",
    "        if constituent.startswith('('+constituent_name):\n",
    "            const_type_list.append(constituent)\n",
    "    \n",
    "    return const_type_list\n",
    "            \n",
    "# E.g. get clauses:   \n",
    "pprint.pprint(get_const_type(const_set, 'SBAR'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(SBAR\n",
      "  (IN because)\n",
      "  (S\n",
      "    (NP (PRP they))\n",
      "    (VP (VBP 're) (VP (VBG ruining) (NP (DT the) (NN market))))))\n",
      "-----------------------------------------\n",
      "(VP\n",
      "  (VB cool)\n",
      "  (NP (PRP it))\n",
      "  (SBAR\n",
      "    (IN because)\n",
      "    (S\n",
      "      (NP (PRP they))\n",
      "      (VP (VBP 're) (VP (VBG ruining) (NP (DT the) (NN market)))))))\n",
      "-----------------------------------------\n",
      "(VP\n",
      "  (VBD told)\n",
      "  (NP (PRP them))\n",
      "  (S\n",
      "    (VP\n",
      "      (TO to)\n",
      "      (VP\n",
      "        (VB cool)\n",
      "        (NP (PRP it))\n",
      "        (SBAR\n",
      "          (IN because)\n",
      "          (S\n",
      "            (NP (PRP they))\n",
      "            (VP\n",
      "              (VBP 're)\n",
      "              (VP (VBG ruining) (NP (DT the) (NN market))))))))))\n",
      "-----------------------------------------\n",
      "(IN because)\n",
      "-----------------------------------------\n",
      "(VP\n",
      "  (TO to)\n",
      "  (VP\n",
      "    (VB cool)\n",
      "    (NP (PRP it))\n",
      "    (SBAR\n",
      "      (IN because)\n",
      "      (S\n",
      "        (NP (PRP they))\n",
      "        (VP (VBP 're) (VP (VBG ruining) (NP (DT the) (NN market))))))))\n",
      "-----------------------------------------\n",
      "(VP\n",
      "  (VBP 've)\n",
      "  (VP\n",
      "    (VP\n",
      "      (VBN talked)\n",
      "      (PP\n",
      "        (TO to)\n",
      "        (NP\n",
      "          (NP (NNS proponents))\n",
      "          (PP (IN of) (NP (NN index) (NN arbitrage))))))\n",
      "    (CC and)\n",
      "    (VP\n",
      "      (VBD told)\n",
      "      (NP (PRP them))\n",
      "      (S\n",
      "        (VP\n",
      "          (TO to)\n",
      "          (VP\n",
      "            (VB cool)\n",
      "            (NP (PRP it))\n",
      "            (SBAR\n",
      "              (IN because)\n",
      "              (S\n",
      "                (NP (PRP they))\n",
      "                (VP\n",
      "                  (VBP 're)\n",
      "                  (VP (VBG ruining) (NP (DT the) (NN market))))))))))))\n",
      "-----------------------------------------\n",
      "(S\n",
      "  (NP (PRP We))\n",
      "  (VP\n",
      "    (VBP 've)\n",
      "    (VP\n",
      "      (VP\n",
      "        (VBN talked)\n",
      "        (PP\n",
      "          (TO to)\n",
      "          (NP\n",
      "            (NP (NNS proponents))\n",
      "            (PP (IN of) (NP (NN index) (NN arbitrage))))))\n",
      "      (CC and)\n",
      "      (VP\n",
      "        (VBD told)\n",
      "        (NP (PRP them))\n",
      "        (S\n",
      "          (VP\n",
      "            (TO to)\n",
      "            (VP\n",
      "              (VB cool)\n",
      "              (NP (PRP it))\n",
      "              (SBAR\n",
      "                (IN because)\n",
      "                (S\n",
      "                  (NP (PRP they))\n",
      "                  (VP\n",
      "                    (VBP 're)\n",
      "                    (VP (VBG ruining) (NP (DT the) (NN market))))))))))))\n",
      "  (. .))\n",
      "-----------------------------------------\n",
      "(VP\n",
      "  (VP\n",
      "    (VBN talked)\n",
      "    (PP\n",
      "      (TO to)\n",
      "      (NP\n",
      "        (NP (NNS proponents))\n",
      "        (PP (IN of) (NP (NN index) (NN arbitrage))))))\n",
      "  (CC and)\n",
      "  (VP\n",
      "    (VBD told)\n",
      "    (NP (PRP them))\n",
      "    (S\n",
      "      (VP\n",
      "        (TO to)\n",
      "        (VP\n",
      "          (VB cool)\n",
      "          (NP (PRP it))\n",
      "          (SBAR\n",
      "            (IN because)\n",
      "            (S\n",
      "              (NP (PRP they))\n",
      "              (VP\n",
      "                (VBP 're)\n",
      "                (VP (VBG ruining) (NP (DT the) (NN market)))))))))))\n",
      "-----------------------------------------\n",
      "(S\n",
      "  (VP\n",
      "    (TO to)\n",
      "    (VP\n",
      "      (VB cool)\n",
      "      (NP (PRP it))\n",
      "      (SBAR\n",
      "        (IN because)\n",
      "        (S\n",
      "          (NP (PRP they))\n",
      "          (VP (VBP 're) (VP (VBG ruining) (NP (DT the) (NN market)))))))))\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Get constituent the connective is part of\n",
    "\n",
    "connective_str = ' '.join(target_connective_pos)\n",
    "\n",
    "\n",
    "for const in const_set:\n",
    "    if connective_str in const:\n",
    "        print(const)\n",
    "        print('-----------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['nsubj', 'talked-3', 'We-1'],\n",
      " ['aux', 'talked-3', \"'ve-2\"],\n",
      " ['root', 'ROOT-0', 'talked-3'],\n",
      " ['prep', 'talked-3', 'to-4'],\n",
      " ['pobj', 'to-4', 'proponents-5'],\n",
      " ['prep', 'proponents-5', 'of-6'],\n",
      " ['nn', 'arbitrage-8', 'index-7'],\n",
      " ['pobj', 'of-6', 'arbitrage-8'],\n",
      " ['cc', 'talked-3', 'and-9'],\n",
      " ['conj', 'talked-3', 'told-10'],\n",
      " ['dobj', 'told-10', 'them-11'],\n",
      " ['aux', 'cool-13', 'to-12'],\n",
      " ['xcomp', 'told-10', 'cool-13'],\n",
      " ['dobj', 'cool-13', 'it-14'],\n",
      " ['mark', 'ruining-18', 'because-15'],\n",
      " ['nsubj', 'ruining-18', 'they-16'],\n",
      " ['aux', 'ruining-18', \"'re-17\"],\n",
      " ['advcl', 'cool-13', 'ruining-18'],\n",
      " ['det', 'market-20', 'the-19'],\n",
      " ['dobj', 'ruining-18', 'market-20']]\n"
     ]
    }
   ],
   "source": [
    "# Get dependencies of a sentence:\n",
    "\n",
    "example_dependencies = parses[DocID]['sentences'][sentence_id_arg1]['dependencies']\n",
    "pprint.pprint(example_dependencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data Structure:\n",
    "\n",
    "Dictionary with unique ID (keys: relation ids, values: lists of dicts)\n",
    "    For each relation:\n",
    "        Connector\n",
    "            Form\n",
    "            PoS\n",
    "            Constituents\n",
    "        Previous sentence\n",
    "            Tokens\n",
    "            Lemma\n",
    "            PoS\n",
    "            Constituents ([CAT, [w1, .. ,wn]])\n",
    "            Dependency\n",
    "        Current sentence\n",
    "            Tokens\n",
    "            Lemma\n",
    "            PoS\n",
    "            Constituents\n",
    "            Dependency\n",
    "        Gold Arg1\n",
    "        Gold Arg2\n",
    "        Relation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
